{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc311b49",
   "metadata": {},
   "source": [
    "---\n",
    "# Analisis Perbandingan antara Cosine Similarity dan SVM dalam Sistem CBR Berbasis TF-IDF untuk Analisis Putusan Pengadilan\n",
    "\n",
    "Anggota Kelompok :\n",
    "\n",
    "1. Muhamad Ahdan Fauzan - 202210370311456\n",
    "\n",
    "2. Khairy Zhafran H. Kastella - 202210370311439\n",
    "\n",
    "## Tugas Besar Mata Kuliah Penalaran Komputer (A)\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b548b8a",
   "metadata": {},
   "source": [
    "# Tahap 1 – Pembangunan Basis Kasus (Case Base)\n",
    "\n",
    "## Tujuan\n",
    "\n",
    "Tahapan ini difokuskan pada pembangunan *case base* awal, yaitu dengan cara mengumpulkan, mengekstrak, serta membersihkan dokumen putusan dari situs resmi Mahkamah Agung Republik Indonesia. Output dari tahap ini adalah teks putusan yang telah terstandardisasi dan siap digunakan dalam proses Case-Based Reasoning (CBR).\n",
    "\n",
    "---\n",
    "\n",
    "## Langkah-Langkah\n",
    "\n",
    "### 1. Pengumpulan dan Seleksi Dokumen\n",
    "\n",
    "* Jenis perkara yang dijadikan fokus: **Pidana Khusus – Narkotika dan Psikotropika** dari **Pengadilan Negeri Bandung**\n",
    "* Sumber resmi: Direktori Putusan Mahkamah Agung RI\n",
    "* Format asli dokumen: PDF\n",
    "* Total dokumen yang dikumpulkan: **35 buah**\n",
    "\n",
    "Seluruh dokumen diunduh secara manual dan disimpan ke dalam direktori `pdf_downloaded/`.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Konversi Dokumen ke Format Teks\n",
    "\n",
    "Setiap dokumen PDF diubah menjadi teks polos dengan bantuan pustaka `pdfminer`. Konversi ini dilakukan agar isi dokumen dapat diolah lebih lanjut secara programatik.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Proses Pembersihan Teks\n",
    "\n",
    "Teks hasil ekstraksi dibersihkan melalui beberapa tahapan, antara lain:\n",
    "\n",
    "* Menghapus watermark, header/footer, serta nomor halaman\n",
    "* Menghilangkan teks disclaimer dari MA RI\n",
    "* Menstandarkan penulisan (huruf kecil, penyesuaian spasi)\n",
    "* Menghitung rasio keutuhan, yaitu perbandingan antara panjang teks bersih dan panjang teks sebelum dibersihkan\n",
    "\n",
    "Hanya dokumen yang memiliki rasio keutuhan minimal **80%** yang disimpan untuk tahap selanjutnya.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Validasi Data dan Pencatatan\n",
    "\n",
    "Seluruh proses pembersihan dicatat ke dalam file log `logs/cleaning.log`, yang mencantumkan rasio keutuhan setiap dokumen. Log ini berfungsi untuk memantau kualitas data dan mengidentifikasi dokumen yang tidak layak pakai.\n",
    "\n",
    "---\n",
    "\n",
    "## Output Tahap Ini\n",
    "\n",
    "- Folder `/data/raw/*.txt` berisi 35 file teks putusan yang telah dibersihkan dan lolos validasi.\n",
    "- File log `/logs/cleaning.log` berisi rekaman validasi keutuhan untuk setiap kasus.\n",
    "- Semua dokumen memiliki rasio keutuhan di atas 88%, menandakan proses ekstraksi dan cleaning berhasil dilakukan dengan baik.\n",
    "\n",
    "Contoh log validasi:\n",
    "[OK] case_001 diproses (89.08% valid)\n",
    "[OK] case_002 diproses (89.29% valid)\n",
    "[OK] case_003 diproses (89.45% valid)\n",
    "...\n",
    "[OK] case_047 diproses (89.34% valid)\n",
    "\n",
    "\n",
    "---\n",
    "Tahap pertama ini berhasil menyiapkan kumpulan kasus dengan kualitas teks yang layak untuk digunakan sebagai basis kasus pada sistem CBR. Tahapan ini menjadi fondasi penting untuk proses representasi dan retrieval pada tahap-tahap selanjutnya.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "749746d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] case_001 diproses (97.42% valid).\n",
      "[OK] case_002 diproses (97.03% valid).\n",
      "[OK] case_003 diproses (97.05% valid).\n",
      "[OK] case_004 diproses (96.98% valid).\n",
      "[OK] case_005 diproses (96.94% valid).\n",
      "[OK] case_006 diproses (96.92% valid).\n",
      "[OK] case_007 diproses (96.92% valid).\n",
      "[OK] case_008 diproses (97.13% valid).\n",
      "[OK] case_009 diproses (96.93% valid).\n",
      "[OK] case_010 diproses (96.93% valid).\n",
      "[OK] case_011 diproses (97.03% valid).\n",
      "[OK] case_012 diproses (97.04% valid).\n",
      "[OK] case_013 diproses (97.09% valid).\n",
      "[OK] case_014 diproses (97.09% valid).\n",
      "[OK] case_015 diproses (97.03% valid).\n",
      "[OK] case_016 diproses (96.96% valid).\n",
      "[OK] case_017 diproses (96.85% valid).\n",
      "[OK] case_018 diproses (96.84% valid).\n",
      "[OK] case_019 diproses (96.97% valid).\n",
      "[OK] case_020 diproses (97.04% valid).\n",
      "[OK] case_021 diproses (97.01% valid).\n",
      "[OK] case_022 diproses (97.02% valid).\n",
      "[OK] case_023 diproses (96.96% valid).\n",
      "[OK] case_024 diproses (97.12% valid).\n",
      "[OK] case_025 diproses (96.99% valid).\n",
      "[OK] case_026 diproses (97.38% valid).\n",
      "[OK] case_027 diproses (96.83% valid).\n",
      "[OK] case_028 diproses (96.95% valid).\n",
      "[OK] case_029 diproses (96.88% valid).\n",
      "[OK] case_030 diproses (97.26% valid).\n",
      "[OK] case_031 diproses (96.82% valid).\n",
      "[OK] case_032 diproses (96.95% valid).\n",
      "[OK] case_033 diproses (97.06% valid).\n",
      "[OK] case_034 diproses (97.10% valid).\n",
      "[OK] case_035 diproses (96.77% valid).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import string  # ← tambahkan ini\n",
    "from pdfminer.high_level import extract_text\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# === Konfigurasi path ===\n",
    "PDF_FOLDER = 'pdf_downloaded'\n",
    "OUTPUT_FOLDER = 'data/raw'\n",
    "LOG_FILE = 'logs/cleaning.log'\n",
    "\n",
    "def clean_text(text):\n",
    "\n",
    "    baseline_length = len(text)\n",
    "   \n",
    "    # Hapus header/footer watermark\n",
    "    text = text.replace(\"Direktori Putusan Mahkamah Agung Republik Indonesia\", \"\")\n",
    "    text = text.replace(\"putusan.mahkamahagung.go.id\", \"\")\n",
    "    text = re.sub(r'halaman\\s*\\d+', '', text, flags=re.IGNORECASE)\n",
    "    text = text.replace(\"M a h ka m a h A g u n g R e p u blik In d o n esia\\n\", \"\")\n",
    "    cleaned_length = len(text)\n",
    "    text = text.replace(\"Disclaimer\\n\", \"\")\n",
    "    text = text.replace(\n",
    "        \"Kepaniteraan Mahkamah Agung Republik Indonesia berusaha untuk selalu mencantumkan informasi paling kini dan akurat sebagai bentuk komitmen Mahkamah Agung untuk pelayanan publik, transparansi dan akuntabilitas\\n\", \"\")\n",
    "    text = text.replace(\n",
    "        \"pelaksanaan fungsi peradilan. Namun dalam hal-hal tertentu masih dimungkinkan terjadi permasalahan teknis terkait dengan akurasi dan keterkinian informasi yang kami sajikan, hal mana akan terus kami perbaiki dari waktu kewaktu.\\n\", \"\")\n",
    "    text = text.replace(\n",
    "        \"Dalam hal Anda menemukan inakurasi informasi yang termuat pada situs ini atau informasi yang seharusnya ada, namun belum tersedia, maka harap segera hubungi Kepaniteraan Mahkamah Agung RI melalui :\\n\", \"\")\n",
    "    text = text.replace(\n",
    "        \"Email : kepaniteraan@mahkamahagung.go.id    Telp : 021-384 3348 (ext.318)\\n\", \"\")\n",
    "    \n",
    "\n",
    "\n",
    "    # Normalisasi akhir\n",
    "    text = text.lower()\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    ratio = cleaned_length / baseline_length \n",
    "\n",
    "    return text, ratio\n",
    "\n",
    "\n",
    "\n",
    "# === Fungsi log (opsional) ===\n",
    "def write_log(case_name, ratio):\n",
    "    os.makedirs(os.path.dirname(LOG_FILE), exist_ok=True)\n",
    "    with open(LOG_FILE, 'a', encoding='utf-8') as f:\n",
    "        timestamp = datetime.now().isoformat()\n",
    "        f.write(f\"[{timestamp}] {case_name} | Integrity: {ratio:.2%}\\n\")\n",
    "\n",
    "# === Proses utama ===\n",
    "def process_pdfs():\n",
    "    os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "    pdf_files = sorted(glob.glob(os.path.join(PDF_FOLDER, '*.pdf')))\n",
    "    \n",
    "    for i, pdf_path in enumerate(pdf_files):\n",
    "        case_id = f\"case_{i+1:03d}\"\n",
    "        output_file = os.path.join(OUTPUT_FOLDER, f\"{case_id}.txt\")\n",
    "\n",
    "        try:\n",
    "            # Ekstrak teks\n",
    "            text = extract_text(pdf_path)\n",
    "\n",
    "            # Bersihkan\n",
    "            cleaned_text, ratio = clean_text(text)\n",
    "\n",
    "            # Validasi keutuhan\n",
    "            if ratio < 0.8:\n",
    "                print(f\"[WARNING] {case_id} hanya {ratio:.2%} isi yang tersisa.\")\n",
    "            else:\n",
    "                print(f\"[OK] {case_id} diproses ({ratio:.2%} valid).\")\n",
    "\n",
    "                # Simpan teks bersih HANYA jika valid\n",
    "                with open(output_file, 'w', encoding='utf-8') as f_out:\n",
    "                    f_out.write(cleaned_text)\n",
    "\n",
    "            # Catat log tetap dicatat semuanya\n",
    "            write_log(case_id, ratio)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Gagal memproses {pdf_path}: {e}\")\n",
    "\n",
    "# === Eksekusi utama ===\n",
    "if __name__ == \"__main__\":\n",
    "    process_pdfs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cdcccf",
   "metadata": {},
   "source": [
    "# Tahap 2 – Case Representation\n",
    "\n",
    "## Tujuan\n",
    "\n",
    "Tahapan ini bertujuan untuk merepresentasikan setiap putusan dalam struktur data yang terorganisir. Hasil representasi ini menjadi basis data terstruktur yang siap digunakan untuk proses retrieval dan analisis lebih lanjut dalam sistem Case-Based Reasoning (CBR).\n",
    "\n",
    "---\n",
    "\n",
    "## Langkah Kerja\n",
    "\n",
    "### 1. Ekstraksi Metadata\n",
    "\n",
    "Setiap dokumen hasil cleaning dianalisis untuk mengekstrak informasi penting sebagai metadata, meliputi:\n",
    "\n",
    "- Nomor Perkara (`no_perkara`)\n",
    "- Tanggal Putusan (`tanggal`)\n",
    "- Ringkasan Fakta (`ringkasan_fakta`)\n",
    "- Pasal yang didakwakan (`pasal`)\n",
    "- Pihak terkait (Terdakwa dan Korban)\n",
    "- Isi teks lengkap (`text_full`)\n",
    "\n",
    "Ekstraksi dilakukan dengan pendekatan berbasis pola (regex) terhadap isi dokumen.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Penyimpanan Data Terstruktur\n",
    "\n",
    "Data hasil ekstraksi disimpan dalam dua format:\n",
    "\n",
    "- **CSV**: `data/processed/cases_extracted.csv`\n",
    "- **JSON**: `data/processed/cases_extracted.json`\n",
    "\n",
    "Struktur kolom yang digunakan meliputi:\n",
    "\n",
    "- `case_id`\n",
    "- `no_perkara`\n",
    "- `tanggal`\n",
    "- `ringkasan_fakta`\n",
    "- `pasal`\n",
    "- `pihak`\n",
    "- `text_full`\n",
    "\n",
    "Jumlah data yang berhasil diproses: **35 kasus**\n",
    "\n",
    "Contoh output terminal:\n",
    "\n",
    "[SUKSES] 35 kasus disimpan ke:\n",
    "\n",
    "CSV → data/processed/cases_extracted.csv\n",
    "\n",
    "JSON → data/processed/cases_extracted.json\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Feature Engineering\n",
    "\n",
    "Untuk meningkatkan pemanfaatan data kasus, dilakukan proses rekayasa fitur (feature engineering) yang meliputi:\n",
    "\n",
    "- **Jumlah Kata (Length)**: Menghitung total token (kata) dalam teks.\n",
    "- **Bag-of-Words (BoW)**: Menghitung frekuensi kata dalam setiap kasus.\n",
    "- **QA-Pairs Sederhana**: Menghasilkan pasangan pertanyaan dan jawaban dari konten teks.\n",
    "\n",
    "QA-Pairs mencakup contoh pertanyaan berikut:\n",
    "\n",
    "- Apa nomor perkaranya?\n",
    "- Apa pasal yang dilanggar?\n",
    "- Siapa terdakwanya?\n",
    "- Siapa korbannya?\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Penyimpanan Fitur\n",
    "\n",
    "Hasil rekayasa fitur disimpan dalam format JSON:\n",
    "\n",
    "- `data/processed/features_length.json`\n",
    "- `data/processed/features_bow.json`\n",
    "- `data/processed/features_qa_pairs.json`\n",
    "\n",
    "Contoh output terminal:\n",
    "\n",
    "[SUKSES] Feature Engineering selesai!\n",
    "\n",
    "Length disimpan di : data/processed/features_length.json\n",
    "\n",
    "Bag-of-Words disimpan di: data/processed/features_bow.json\n",
    "\n",
    "QA-pairs disimpan di : data/processed/features_qa_pairs.json\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Tahap representasi berhasil membentuk struktur data terorganisir untuk 35 kasus. Setiap kasus dilengkapi metadata, ringkasan fakta, dan fitur tambahan untuk mendukung proses retrieval dan prediksi pada tahapan selanjutnya dalam sistem CBR.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bcfbd997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUKSES] 35 kasus disimpan ke:\n",
      "- CSV  → data/processed/cases_extracted.csv\n",
      "- JSON → data/processed/cases_extracted.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# === Path Konfigurasi ===\n",
    "RAW_FOLDER  = 'data/raw'\n",
    "CSV_OUTPUT  = 'data/processed/cases_extracted.csv'\n",
    "JSON_OUTPUT = 'data/processed/cases_extracted.json'\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "\n",
    "# === Fungsi pencarian hasil pertama ===\n",
    "def find_first(pattern, text):\n",
    "    hits = re.findall(pattern, text, re.IGNORECASE | re.DOTALL)\n",
    "    return hits[0].strip() if hits else \"\"\n",
    "\n",
    "# === Fungsi membersihkan nilai CSV ===\n",
    "def clean_for_csv(text):\n",
    "    return \"\" if not text else text.replace(\",\", \" \").replace(\"\\n\", \" \").strip()\n",
    "\n",
    "# === Ekstraksi data per dokumen ===\n",
    "def extract_case_data(case_id, text):\n",
    "    text_clean = \" \".join(text.split())\n",
    "\n",
    "    no_perkara = clean_for_csv(find_first(r\"p\\s*u\\s*t\\s*u\\s*s\\s*a\\s*n\\s*\\\"?(.*?)\\\"?\\s*demi keadilan\", text_clean))\n",
    "    tanggal    = find_first(r'pada hari\\s+\\w+\\s+tanggal\\s+(\\d{1,2}\\s+\\w+\\s+\\d{4})', text_clean)\n",
    "\n",
    "    # ---------- Ringkasan fakta: 4 opsi ----------\n",
    "    patterns_fakta = [\n",
    "        r'ditemukan\\s+barang\\s+bukti\\s+berupa\\s*(.*?)(?=\\n\\s*\\n|menimbang)',                 # opsi 1\n",
    "        r'bahwa\\s+benar\\s+barang\\s+bukti\\s*(.*?)(?=\\n\\s*\\n|menimbang)',                     # opsi 2\n",
    "        r'barang\\s+bukti\\s+berupa\\s*(.*?)(?=\\n\\s*\\n|menimbang)',                            # opsi 3\n",
    "        r'bahwa\\s+terhadap\\s+barang\\s+bukti\\s+dalam\\s+perkara\\s*(.*?)(?=\\n\\s*\\n|menimbang)' # opsi 4\n",
    "    ]\n",
    "    ringkasan_fakta = \"\"\n",
    "    for pat in patterns_fakta:\n",
    "        ringkasan_fakta = find_first(pat, text_clean)\n",
    "        if ringkasan_fakta:\n",
    "            break\n",
    "\n",
    "    pasal = find_first(r'(pasal.+?narkotika)', text_clean)\n",
    "\n",
    "    # --- Pihak terdakwa / saksi ---\n",
    "    m_mengadili = re.search(r'mengadili\\s*:\\s*1\\.', text_clean, re.IGNORECASE)\n",
    "    after_mengadili = text_clean[m_mengadili.end():] if m_mengadili else text_clean\n",
    "    pihak_terdakwa  = find_first(\n",
    "        r'menyatakan(?: bahwa)?(?: ia)?\\s+(?:terdakwa|saksi)\\s+(.+?)\\s+(?:terbukti|telah|tersebut)',\n",
    "        after_mengadili\n",
    "    )\n",
    "    korban_match = find_first(r'korban\\s+([a-zA-Z]+)', text_clean)\n",
    "    pihak = f\"Terdakwa: {pihak_terdakwa} Korban: {korban_match}\" if (pihak_terdakwa or korban_match) else \"\"\n",
    "\n",
    "    return {\n",
    "        \"case_id\":        case_id,\n",
    "        \"no_perkara\":     clean_for_csv(no_perkara),\n",
    "        \"tanggal\":        clean_for_csv(tanggal),\n",
    "        \"ringkasan_fakta\":clean_for_csv(ringkasan_fakta),\n",
    "        \"pasal\":          clean_for_csv(pasal),\n",
    "        \"pihak\":          clean_for_csv(pihak),\n",
    "        \"text_full\":      clean_for_csv(text)\n",
    "    }\n",
    "\n",
    "# === Proses semua dokumen ===\n",
    "cases = []\n",
    "for i in range(1, 48):                      # case_001.txt – case_047.txt\n",
    "    fp = os.path.join(RAW_FOLDER, f\"case_{i:03}.txt\")\n",
    "    if not os.path.exists(fp):\n",
    "        continue\n",
    "    with open(fp, encoding='utf-8') as f:\n",
    "        cases.append(extract_case_data(i, f.read()))\n",
    "\n",
    "# === Simpan hasil ===\n",
    "pd.DataFrame(cases).to_csv(CSV_OUTPUT, index=False)\n",
    "with open(JSON_OUTPUT, 'w', encoding='utf-8') as jf:\n",
    "    json.dump(cases, jf, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"[SUKSES] {len(cases)} kasus disimpan ke:\")\n",
    "print(f\"- CSV  → {CSV_OUTPUT}\")\n",
    "print(f\"- JSON → {JSON_OUTPUT}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f2b4bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUKSES] Feature Engineering selesai!\n",
      "- Length disimpan di      : data/processed\\features_length.json\n",
      "- Bag-of-Words disimpan di: data/processed\\features_bow.json\n",
      "- QA-pairs disimpan di    : data/processed\\features_qa_pairs.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# === Path ===\n",
    "RAW_FOLDER = 'data/raw'\n",
    "PROCESSED_FOLDER = 'data/processed'\n",
    "os.makedirs(PROCESSED_FOLDER, exist_ok=True)\n",
    "\n",
    "# === File Output Feature Engineering ===\n",
    "LENGTH_FILE = os.path.join(PROCESSED_FOLDER, 'features_length.json')\n",
    "BOW_FILE = os.path.join(PROCESSED_FOLDER, 'features_bow.json')\n",
    "QA_FILE = os.path.join(PROCESSED_FOLDER, 'features_qa_pairs.json')\n",
    "\n",
    "# === Tokenizer sederhana ===\n",
    "def tokenize(text):\n",
    "    return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "# === Ekstraksi QA pairs ===\n",
    "def get_qa_pairs(text):\n",
    "    return {\n",
    "        \"Apa nomor perkaranya?\": re.search(r\"putusan\\s+\\\"?([^\\\"\\,\\n]+)\", text, re.IGNORECASE | re.DOTALL),\n",
    "        \"Apa pasal yang dilanggar?\": re.search(r\"melanggar pasal\\s+\\\"?([^\\\"\\,\\.\\;\\:]+)\", text, re.IGNORECASE),\n",
    "        \"Siapa terdakwanya?\": re.search(r\"terdakwa\\s+([a-zA-Z]+)\", text, re.IGNORECASE),\n",
    "        \"Siapa korbannya?\": re.search(r\"korban\\s+([a-zA-Z]+)\", text, re.IGNORECASE),\n",
    "    }\n",
    "\n",
    "# === Proses semua file ===\n",
    "length_data = {}\n",
    "bow_data = {}\n",
    "qa_data = {}\n",
    "\n",
    "for i in range(1, 48):  # case_001.txt - case_047.txt\n",
    "    file_path = os.path.join(RAW_FOLDER, f\"case_{i:03}.txt\")\n",
    "    if not os.path.exists(file_path):\n",
    "        continue\n",
    "\n",
    "    with open(file_path, encoding='utf-8') as f:\n",
    "        raw_text = f.read()\n",
    "\n",
    "    text_clean = \" \".join(raw_text.split())\n",
    "    tokens = tokenize(text_clean)\n",
    "    case_id = f\"case_{i:03}\"\n",
    "\n",
    "    # Jumlah kata\n",
    "    length_data[case_id] = len(tokens)\n",
    "\n",
    "    # Bag-of-Words\n",
    "    bow_data[case_id] = dict(Counter(tokens))\n",
    "\n",
    "    # QA Pairs\n",
    "    qas = get_qa_pairs(text_clean)\n",
    "    qa_data[case_id] = {q: m.group(1).strip() if m else None for q, m in qas.items()}\n",
    "\n",
    "# === Simpan ke file JSON terpisah ===\n",
    "with open(LENGTH_FILE, 'w', encoding='utf-8') as f:\n",
    "    json.dump(length_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(BOW_FILE, 'w', encoding='utf-8') as f:\n",
    "    json.dump(bow_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(QA_FILE, 'w', encoding='utf-8') as f:\n",
    "    json.dump(qa_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"[SUKSES] Feature Engineering selesai!\")\n",
    "print(f\"- Length disimpan di      : {LENGTH_FILE}\")\n",
    "print(f\"- Bag-of-Words disimpan di: {BOW_FILE}\")\n",
    "print(f\"- QA-pairs disimpan di    : {QA_FILE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131ef579",
   "metadata": {},
   "source": [
    "# Tahap 3 – Case Retrieval\n",
    "\n",
    "## Tujuan\n",
    "\n",
    "Tahap ini bertujuan untuk menemukan kasus-kasus lama yang paling relevan dan mirip dengan query kasus baru yang diajukan. Proses ini merupakan bagian utama dalam sistem Case-Based Reasoning (CBR) untuk mendukung analisis dan pencarian preseden hukum.\n",
    "\n",
    "---\n",
    "\n",
    "## Langkah Kerja\n",
    "\n",
    "### 1. Representasi Vektor\n",
    "\n",
    "- Setiap ringkasan fakta dari putusan diubah menjadi representasi vektor menggunakan algoritma **TF-IDF** (`TfidfVectorizer` dari `sklearn`).\n",
    "- Alternatif lain yang tersedia namun tidak digunakan pada tahap ini adalah embedding berbasis **transformer** seperti **IndoBERT**.\n",
    "\n",
    "### 2. Splitting Data\n",
    "\n",
    "- Dataset dibagi menjadi dua bagian: **data latih (train)** dan **data uji (test)** dengan rasio **80:20**.\n",
    "- Teknik ini digunakan untuk pelatihan model klasifikasi berbasis TF-IDF + SVM.\n",
    "\n",
    "### 3. Model Retrieval\n",
    "\n",
    "Dalam tahap ini, sistem dibangun menggunakan **dua pendekatan berbeda** untuk melakukan retrieval terhadap kasus lama yang paling relevan dengan query baru:\n",
    "\n",
    "#### a. TF-IDF + Cosine Similarity (Pendekatan Case-Based Reasoning)\n",
    "\n",
    "- Menggunakan **TF-IDF vectorizer** untuk merepresentasikan teks ringkasan fakta dari semua kasus sebagai vektor numerik.\n",
    "- Query kasus baru juga diubah menjadi vektor menggunakan TF-IDF yang sama.\n",
    "- Kemiripan antar vektor dihitung menggunakan **cosine similarity**.\n",
    "- Top-k kasus dengan skor kemiripan tertinggi dipilih sebagai hasil retrieval.\n",
    "- Pendekatan ini bersifat **unsupervised** dan murni berbasis kemiripan teks.\n",
    "\n",
    "#### b. TF-IDF + Support Vector Machine (SVM) (Pendekatan Supervised Classification)\n",
    "\n",
    "- Menggunakan **TF-IDF vectorizer** untuk mengubah ringkasan fakta menjadi fitur numerik.\n",
    "- Menggunakan model **LinearSVC (SVM)** dari `sklearn` yang dilatih secara supervised dengan `case_id` sebagai label target.\n",
    "- Model mempelajari pola dari data latih dan digunakan untuk memprediksi satu kasus (case_id) yang paling cocok dengan query baru.\n",
    "- Pendekatan ini bersifat **supervised learning** dan menekankan pada klasifikasi.\n",
    "\n",
    "Kedua pendekatan digunakan untuk saling melengkapi dalam proses evaluasi performa sistem pada tahap selanjutnya.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Fungsi Retrieval\n",
    "\n",
    "Dua fungsi `retrieve()` disiapkan, masing-masing untuk kedua pendekatan:\n",
    "\n",
    "- Pada **pendekatan TF-IDF + Cosine**, fungsi `retrieve(query: str, k: int = 5)` akan:\n",
    "  1. Mengubah query menjadi vektor TF-IDF.\n",
    "  2. Menghitung cosine similarity dengan seluruh vektor kasus.\n",
    "  3. Mengembalikan **top-k case_id** dengan skor tertinggi.\n",
    "\n",
    "- Pada **pendekatan TF-IDF + SVM**, fungsi `retrieve(query: str, k: int = 1)` akan:\n",
    "  1. Mengubah query menjadi vektor TF-IDF.\n",
    "  2. Melakukan klasifikasi menggunakan model SVM.\n",
    "  3. Mengembalikan **case_id hasil prediksi** dari model (top-1).\n",
    "\n",
    "---\n",
    "\n",
    "Dengan pendekatan ganda ini, sistem mampu membandingkan efektivitas metode retrieval berbasis kemiripan teks dengan metode klasifikasi berbasis pembelajaran mesin.\n",
    "\n",
    "\n",
    "### 5. Pengujian Awal\n",
    "\n",
    "- Disiapkan **10 query uji** beserta **ground truth** (case ID yang dianggap paling relevan).\n",
    "- Query dan ground truth disimpan ke file `data/eval/queries.json` untuk keperluan evaluasi pada tahap selanjutnya.\n",
    "\n",
    "---\n",
    "\n",
    "## Output\n",
    "\n",
    "- Model klasifikasi berbasis SVM disimpan di:  \n",
    "  `03_retrieval_model.pkl`\n",
    "- Vectorizer TF-IDF disimpan di:  \n",
    "  `03_vectorizer.pkl`\n",
    "- Dataset query uji disimpan di:  \n",
    "  `data/eval/queries.json`\n",
    "\n",
    "Contoh output terminal:\n",
    "\n",
    "[SUKSES] Tahap 3 Case Retrieval selesai:\n",
    "\n",
    "Model disimpan di : 03_retrieval_model.pkl\n",
    "\n",
    "Vectorizer disimpan di : 03_vectorizer.pkl\n",
    "\n",
    "10 query uji disimpan di : data/eval/queries.json\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Tahap Case Retrieval telah berhasil diimplementasikan menggunakan pendekatan supervised classification berbasis **TF-IDF + SVM**. Model ini siap digunakan untuk tahap prediksi (Solution Reuse) dan evaluasi performa pada tahap selanjutnya.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d29e433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUKSES] Tahap 3 Case Retrieval selesai:\n",
      "- Model disimpan di        : 03_retrieval_model.pkl\n",
      "- Vectorizer disimpan di   : 03_vectorizer.pkl\n",
      "- 10 query uji disimpan di : data/eval/queries.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "\n",
    "# === PATH KONFIGURASI ===\n",
    "DATA_JSON = \"data/processed/cases_extracted.json\"\n",
    "MODEL_OUTPUT = \"03_retrieval_model.pkl\"\n",
    "VECTORIZER_OUTPUT = \"03_vectorizer.pkl\"\n",
    "EVAL_QUERIES = \"data/eval/queries.json\"\n",
    "os.makedirs(\"data/eval\", exist_ok=True)\n",
    "\n",
    "# === LOAD DATASET JSON ===\n",
    "with open(DATA_JSON, encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "case_ids = [case[\"case_id\"] for case in data]\n",
    "texts = [case[\"ringkasan_fakta\"] for case in data]\n",
    "\n",
    "# === SPLIT DATA 80:20 ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    texts, case_ids, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# === TRAIN TF-IDF + SVM ===\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "classifier = LinearSVC()\n",
    "classifier.fit(X_train_vec, y_train)\n",
    "\n",
    "# === SIMPAN MODEL DAN VECTORIZER ===\n",
    "joblib.dump(classifier, MODEL_OUTPUT)\n",
    "joblib.dump(vectorizer, VECTORIZER_OUTPUT)\n",
    "\n",
    "# === FUNGSI RETRIEVE MENGGUNAKAN MODEL SVM ===\n",
    "def retrieve(query: str, k: int = 1) -> List[int]:\n",
    "    query_vec = vectorizer.transform([query])\n",
    "    prediction = classifier.predict(query_vec)\n",
    "    return prediction.tolist()\n",
    "\n",
    "# === PENGUJIAN AWAL: 10 QUERY UJI ===\n",
    "sample_queries = [\n",
    "    {\n",
    "        \"query\": \"narkotika  namun setelah dilakukan penggeledahan terhadap terdakwa cahyadi als okep ditemukan barang bukti berupa 1 (satu) buah tas warna hitam berisikan 12 (dua belas) paket narkotika jenis sabu...\",\n",
    "        \"ground_truth\": [1, 2, 3]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"pada saat dilakukan penggeledahan terhadap terdakwa ditemukan 1 bungkus plastik bening narkotika jenis sabu di dalam kamar kontrakan...\",\n",
    "        \"ground_truth\": [2, 3, 4]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"terdakwa mengakui menyimpan ekstasi sebanyak 50 butir di lemari rumahnya setelah mendapatkannya dari seseorang di Jakarta...\",\n",
    "        \"ground_truth\": [3, 4, 5]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"terdakwa menjadi kurir narkoba lintas kota untuk mengedarkan sabu atas perintah seseorang bernama Eka (DPO)...\",\n",
    "        \"ground_truth\": [4, 5, 6]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"mahasiswa ditangkap setelah terbukti menjual tembakau sintetis via media sosial dengan bukti chat pemesanan...\",\n",
    "        \"ground_truth\": [5, 6, 7]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"terdakwa kedapatan menanam ganja di halaman rumah belakang dan mengaku untuk konsumsi pribadi...\",\n",
    "        \"ground_truth\": [6, 7, 8]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"sabu ditemukan dalam bungkus teh cina di dalam koper milik terdakwa saat dilakukan pemeriksaan bandara...\",\n",
    "        \"ground_truth\": [7, 8, 9]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"pelajar berperan menjadi perantara jual beli narkoba jenis sabu dengan upah 500 ribu rupiah per gram...\",\n",
    "        \"ground_truth\": [8, 9, 10]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"terdakwa merupakan bagian dari jaringan internasional yang menyuplai sabu ke beberapa kota besar di Indonesia...\",\n",
    "        \"ground_truth\": [9, 10, 11]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"kokain ditemukan di dalam dashboard mobil yang dikendarai oleh terdakwa saat razia malam di tol Cipularang...\",\n",
    "        \"ground_truth\": [10, 11, 12]\n",
    "    }\n",
    "]\n",
    "\n",
    "# === SIMPAN QUERY UJI KE FILE JSON ===\n",
    "with open(EVAL_QUERIES, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(sample_queries, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"[SUKSES] Tahap 3 Case Retrieval selesai:\")\n",
    "print(f\"- Model disimpan di        : {MODEL_OUTPUT}\")\n",
    "print(f\"- Vectorizer disimpan di   : {VECTORIZER_OUTPUT}\")\n",
    "print(f\"- 10 query uji disimpan di : {EVAL_QUERIES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3651cd01",
   "metadata": {},
   "source": [
    "### Tahap 4 – Pemanfaatan Solusi Terdahulu\n",
    "\n",
    "#### Tujuan\n",
    "\n",
    "Tahapan ini bertujuan untuk menggunakan solusi dari kasus sebelumnya (putusan pengadilan) sebagai acuan atau dasar dalam memprediksi penyelesaian untuk kasus baru yang memiliki kemiripan.\n",
    "\n",
    "---\n",
    "\n",
    "#### Prosedur Pelaksanaan\n",
    "\n",
    "1. **Pengambilan Solusi**\n",
    "\n",
    "   * Dari setiap kasus terdahulu yang berhasil diambil, sistem mengekstraksi bagian **amar putusan** atau **keseluruhan isi putusan** sebagai bentuk penyelesaian.\n",
    "   * Hasil ekstraksi disimpan dalam struktur data dictionary dengan format `{case_id: solusi_text}`.\n",
    "\n",
    "2. **Metode Prediksi Solusi**\n",
    "   Sistem mendukung dua pendekatan untuk memprediksi solusi berdasarkan kasus yang serupa:\n",
    "\n",
    "   * **Pendekatan CBR: TF-IDF + Cosine Similarity**\n",
    "\n",
    "     * Sistem memilih sejumlah kasus terdekat berdasarkan nilai cosine similarity dari representasi vektor TF-IDF.\n",
    "     * Solusi dari kasus yang memiliki kemiripan tertinggi (peringkat teratas/top-1) digunakan sebagai `predicted_solution`.\n",
    "\n",
    "   * **Pendekatan Supervised: TF-IDF + SVM**\n",
    "\n",
    "     * Model klasifikasi SVM digunakan untuk memprediksi ID kasus yang paling relevan.\n",
    "     * Amar putusan dari kasus hasil prediksi tersebut dijadikan sebagai `predicted_solution`.\n",
    "\n",
    "3. **Penyajian Ringkas Solusi**\n",
    "\n",
    "   * Untuk menjaga kejelasan dan efisiensi penyajian, solusi yang diprediksi diringkas menjadi paragraf pendek (sekitar 50 kata pertama), menyerupai bentuk abstrak.\n",
    "\n",
    "4. **Simulasi Manual**\n",
    "\n",
    "   * Sebanyak 10 kasus baru digunakan sebagai contoh pengujian.\n",
    "   * Setiap query diuji menggunakan fungsi `predict_outcome()`, lalu hasil prediksi dibandingkan dengan konteks masalah dari kasus tersebut.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### Fungsi Utama\n",
    "\n",
    "- `retrieve(query: str, k: int = 5)`: Mengambil top-k `case_id` berdasarkan pendekatan yang digunakan (Cosine atau SVM).\n",
    "- `predict_outcome(query: str) -> Tuple[str, List[int]]`: Mengembalikan ringkasan solusi prediksi dan daftar case_id yang digunakan sebagai referensi.\n",
    "\n",
    "---\n",
    "\n",
    "#### Output\n",
    "\n",
    "Dua file hasil prediksi disimpan dalam direktori:\n",
    "\n",
    "- `data/results/predictions_cosine.csv`: Berisi hasil prediksi menggunakan **TF-IDF + Cosine Similarity**.\n",
    "- `data/results/predictions_svm.csv`: Berisi hasil prediksi menggunakan **TF-IDF + SVM**.\n",
    "\n",
    "Setiap file prediksi mencakup kolom:\n",
    "- `query_id`: Nomor urut query.\n",
    "- `query`: Ringkasan kasus baru.\n",
    "- `predicted_solution`: Ringkasan solusi yang diprediksi dari kasus lama.\n",
    "- `top_5_case_ids`: Daftar `case_id` dari kasus lama yang dijadikan referensi.\n",
    "\n",
    "Contoh struktur isi file prediksi:\n",
    "\n",
    "query_id,query,predicted_solution,top_5_case_ids\n",
    "1,narkotika  namun setelah dilakukan penggeledahan terhadap terdakwa cahyadi als okep ditemukan barang...,mahkamah agung republik indonesia mahkamah agung republik indonesia mahkamah agung republik indonesia mahkamah agung republik indonesia mahkamah agung republik indonesia halamat 1 dari 70 halamn putusan nomor : 176/pid.sus/2025/pn.bdg pengadilan negeri bandung kl. ia khusus p u t u s a n nomor 176/pid.sus/2025/pn.bdg “demi keadilan berdasarkan ketuhanan yang maha...,\"[1, 3, 20, 2, 25]\"\n",
    "2,pada saat dilakukan penggeledahan terhadap terdakwa ditemukan 1 bungkus plastik bening narkotika jen...,mahkamah agung republik indonesia mahkamah agung republik indonesia mahkamah agung republik indonesia mahkamah agung republik indonesia mahkamah agung republik indonesia dari 24 putusan nomor 255/pid.sus/2025/pn bdg p u t u s a n nomor 255/pid.sus/2025/pn bdg demi keadilan berdasarkan ketuhanan yang maha esa pengadilan negeri bandung yang mengadili perkara pidana...,\"[9, 10, 2, 25, 8]\"\n",
    "3,terdakwa mengakui menyimpan ekstasi sebanyak 50 butir di lemari rumahnya setelah mendapatkannya dari...,mahkamah agung republik indonesia mahkamah agung republik indonesia mahkamah agung republik indonesia mahkamah agung republik indonesia mahkamah agung republik indonesia dari 23 halaman putusan nomor 256/pid sus/2025/pn bdg p u t u s a n nomor 256/pid.sus/2025/pn bdg demi keadilan berdasarkan ketuhanan yang maha esa pengadilan negeri bandung yang mengadili...,\"[11, 35, 8, 15, 25]\"\n",
    "...\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### Catatan\n",
    "Dengan adanya dua pendekatan (unsupervised dan supervised), sistem dapat dibandingkan performanya pada tahap evaluasi berikutnya untuk melihat model mana yang paling efektif dalam memanfaatkan solusi dari kasus terdahulu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f904ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUKSES] Tahap 4 - Solution Reuse selesai\n",
      "- Hasil prediksi disimpan di: data/results/predictions_cosine.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# === PATH ===\n",
    "DATA_JSON = \"data/processed/cases_extracted.json\"\n",
    "MODEL_PATH = \"03_retrieval_model.pkl\"\n",
    "VECTORIZER_PATH = \"03_vectorizer.pkl\"\n",
    "EVAL_QUERIES = \"data/eval/queries.json\"\n",
    "PREDICTION_CSV = \"data/results/predictions_cosine.csv\"\n",
    "os.makedirs(\"data/results\", exist_ok=True)\n",
    "\n",
    "# === LOAD MODEL DAN VECTORIZER ===\n",
    "model = joblib.load(MODEL_PATH)\n",
    "vectorizer = joblib.load(VECTORIZER_PATH)\n",
    "\n",
    "# === LOAD CASE DATA ===\n",
    "with open(DATA_JSON, encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Buat struktur case_id → solusi (di sini pakai text_full sebagai solusi)\n",
    "case_solutions: Dict[int, str] = {case[\"case_id\"]: case[\"text_full\"] for case in data}\n",
    "\n",
    "# Siapkan data TF-IDF untuk semua kasus\n",
    "texts = [case[\"ringkasan_fakta\"] for case in data]\n",
    "case_ids = [case[\"case_id\"] for case in data]\n",
    "X_all = vectorizer.transform(texts)\n",
    "\n",
    "# === RETRIEVE DENGAN COSINE SIMILARITY BERDASARKAN TF-IDF MODEL ===\n",
    "def retrieve(query: str, k: int = 5) -> List[int]:\n",
    "    query_vec = vectorizer.transform([query])\n",
    "    sims = cosine_similarity(query_vec, X_all).flatten()\n",
    "    top_indices = sims.argsort()[::-1][:k]\n",
    "    return [case_ids[i] for i in top_indices]\n",
    "\n",
    "# === SOLUTION REUSE FUNCTION ===\n",
    "def predict_outcome(query: str, k: int = 5) -> str:\n",
    "    top_k = retrieve(query, k)\n",
    "    solutions = [case_solutions[cid] for cid in top_k]\n",
    "    most_common = solutions[0]\n",
    "    # Ringkas solusi seperti abstrak\n",
    "    predicted_summary = \" \".join(most_common.split()[:50]) + \"...\"\n",
    "    return predicted_summary, top_k\n",
    "\n",
    "# === MUAT QUERY UJI ===\n",
    "with open(EVAL_QUERIES, encoding='utf-8') as f:\n",
    "    eval_queries = json.load(f)\n",
    "\n",
    "# === PROSES DAN SIMPAN HASIL ===\n",
    "prediction_rows = []\n",
    "for i, item in enumerate(eval_queries, 1):\n",
    "    query = item[\"query\"]\n",
    "    predicted_solution, top_5_ids = predict_outcome(query)\n",
    "    short_query = query[:100] + \"...\" if len(query) > 100 else query\n",
    "    prediction_rows.append({\n",
    "        \"query_id\": i,\n",
    "        \"query\": short_query,\n",
    "        \"predicted_solution\": predicted_solution,\n",
    "        \"top_5_case_ids\": top_5_ids\n",
    "    })\n",
    "\n",
    "# Simpan ke CSV\n",
    "pd.DataFrame(prediction_rows).to_csv(PREDICTION_CSV, index=False)\n",
    "\n",
    "print(\"[SUKSES] Tahap 4 - Solution Reuse selesai\")\n",
    "print(f\"- Hasil prediksi disimpan di: {PREDICTION_CSV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "802799ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUKSES] Tahap 4 - Solution Reuse selesai\n",
      "- Hasil prediksi disimpan di: data/results/predictions_svm.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "\n",
    "# === PATH ===\n",
    "DATA_JSON = \"data/processed/cases_extracted.json\"\n",
    "MODEL_PATH = \"03_retrieval_model.pkl\"\n",
    "VECTORIZER_PATH = \"03_vectorizer.pkl\"\n",
    "EVAL_QUERIES = \"data/eval/queries.json\"\n",
    "PREDICTION_CSV = \"data/results/predictions_svm.csv\"\n",
    "os.makedirs(\"data/results\", exist_ok=True)\n",
    "\n",
    "# === LOAD MODEL DAN VECTORIZER ===\n",
    "model = joblib.load(MODEL_PATH)\n",
    "vectorizer = joblib.load(VECTORIZER_PATH)\n",
    "\n",
    "# === LOAD CASE DATA ===\n",
    "with open(DATA_JSON, encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Buat struktur case_id → solusi (di sini pakai text_full sebagai solusi)\n",
    "case_solutions: Dict[int, str] = {case[\"case_id\"]: case[\"text_full\"] for case in data}\n",
    "\n",
    "# Siapkan data untuk indexing\n",
    "texts = [case[\"ringkasan_fakta\"] for case in data]\n",
    "case_ids = [case[\"case_id\"] for case in data]\n",
    "\n",
    "# === RETRIEVE DENGAN SVM ===\n",
    "def retrieve(query: str) -> List[int]:\n",
    "    query_vec = vectorizer.transform([query])\n",
    "    prediction = model.predict(query_vec)  # klasifikasi supervised, hasil satu case_id\n",
    "    return prediction.tolist()  # hasilnya list dengan 1 item\n",
    "\n",
    "# === SOLUTION REUSE FUNCTION ===\n",
    "def predict_outcome(query: str) -> str:\n",
    "    top_1 = retrieve(query)  # hasil dari model SVM\n",
    "    solutions = [case_solutions[cid] for cid in top_1]\n",
    "    predicted_summary = \" \".join(solutions[0].split()[:50]) + \"...\"  # Ringkas abstrak\n",
    "    return predicted_summary, top_1\n",
    "\n",
    "# === MUAT QUERY UJI ===\n",
    "with open(EVAL_QUERIES, encoding='utf-8') as f:\n",
    "    eval_queries = json.load(f)\n",
    "\n",
    "# === PROSES DAN SIMPAN HASIL ===\n",
    "prediction_rows = []\n",
    "for i, item in enumerate(eval_queries, 1):\n",
    "    query = item[\"query\"]\n",
    "    predicted_solution, top_ids = predict_outcome(query)\n",
    "    short_query = query[:100] + \"...\" if len(query) > 100 else query\n",
    "    prediction_rows.append({\n",
    "        \"query_id\": i,\n",
    "        \"query\": short_query,\n",
    "        \"predicted_solution\": predicted_solution,\n",
    "        \"top_5_case_ids\": top_ids\n",
    "    })\n",
    "\n",
    "# Simpan ke CSV\n",
    "pd.DataFrame(prediction_rows).to_csv(PREDICTION_CSV, index=False)\n",
    "\n",
    "print(\"[SUKSES] Tahap 4 - Solution Reuse selesai\")\n",
    "print(f\"- Hasil prediksi disimpan di: {PREDICTION_CSV}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1fa831",
   "metadata": {},
   "source": [
    "## Tahap 5 – Evaluasi Model\n",
    "\n",
    "### Tujuan\n",
    "\n",
    "Tahapan ini difokuskan untuk menilai dan menganalisis performa sistem dalam proses pencarian (*retrieval*) dan prediksi solusi atas kasus baru dengan mengacu pada putusan dari kasus-kasus sebelumnya.\n",
    "\n",
    "---\n",
    "\n",
    "### Prosedur\n",
    "\n",
    "#### 1. Evaluasi Proses Retrieval\n",
    "\n",
    "Evaluasi dilakukan terhadap dua pendekatan berbeda, yaitu:\n",
    "\n",
    "* **TF-IDF dipadukan dengan Cosine Similarity (unsupervised/CBR)**\n",
    "* **TF-IDF dipadukan dengan SVM (klasifikasi terawasi/supervised)**\n",
    "\n",
    "Metode evaluasi menggunakan empat metrik utama:\n",
    "\n",
    "* **Akurasi (Accuracy)**\n",
    "* **Presisi (Precision)**\n",
    "* **Recall**\n",
    "* **Skor F1 (F1-score)**\n",
    "\n",
    "Proses evaluasi dilakukan dengan membandingkan hasil prediksi `top_k` terhadap `case_id` yang benar (ground truth) dari 10 kasus uji.\n",
    "\n",
    "#### 2. Visualisasi dan Dokumentasi\n",
    "\n",
    "* Hasil evaluasi ditampilkan dalam bentuk grafik batang (*bar chart*) untuk membandingkan performa kedua model.\n",
    "* Selain itu, sistem juga melakukan analisis terhadap kasus-kasus yang salah diprediksi (*error analysis*), dan menyimpan datanya dalam format `.json`.\n",
    "\n",
    "---\n",
    "\n",
    "### Implementasi Teknis\n",
    "\n",
    "Tiga fungsi utama yang digunakan dalam tahap evaluasi adalah sebagai berikut:\n",
    "\n",
    "* `eval_retrieval()`: Mengevaluasi hasil dari metode *cosine similarity* terhadap beberapa hasil teratas (top-k).\n",
    "* `eval_prediction()`: Menganalisis akurasi prediksi tunggal (top-1) dari model klasifikasi SVM.\n",
    "* `save_errors()`: Mendokumentasikan kasus-kasus yang tidak terklasifikasi dengan tepat.\n",
    "\n",
    "---\n",
    "\n",
    "### Hasil Keluaran\n",
    "\n",
    "#### 1. Berkas Evaluasi\n",
    "\n",
    "* **Evaluasi Retrieval dengan Cosine Similarity**\n",
    "  Lokasi penyimpanan: `data/eval/retrieval_metrics.csv`\n",
    "\n",
    "```\n",
    "model,accuracy,precision,recall,f1_score  \n",
    "TF-IDF + Cosine,0.9,1.0,0.9,0.9473684210526315\n",
    "```\n",
    "\n",
    "* **Evaluasi Prediksi Menggunakan SVM**\n",
    "  Lokasi penyimpanan: `data/eval/prediction_metrics.csv`\n",
    "\n",
    "```\n",
    "model,accuracy,precision,recall,f1_score  \n",
    "TF-IDF + SVM,0.6,1.0,0.6,0.75\n",
    "```\n",
    "\n",
    "#### 2. Hasil Visualisasi\n",
    "\n",
    "* Grafik perbandingan performa model-model tersebut dapat ditemukan pada:\n",
    "  `data/eval/performance_comparison.png`\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "#### 3. Error Analysis\n",
    "\n",
    "- **Kesalahan pada TF-IDF + Cosine:**\n",
    "  Disimpan di: `data/eval/error_cases_cosine.json`\n",
    "  ```json\n",
    "  [\n",
    "    {\n",
    "      \"query_id\": 3,\n",
    "      \"query\": \"terdakwa mengakui menyimpan ekstasi sebanyak 50 butir di lemari rumahnya setelah mendapatkannya dari...\",\n",
    "      \"predicted\": [\n",
    "        11,\n",
    "        35,\n",
    "        8,\n",
    "        15,\n",
    "        25\n",
    "      ],\n",
    "      \"ground_truth\": [\n",
    "        3,\n",
    "        4,\n",
    "        5\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"query_id\": 4,\n",
    "      \"query\": \"terdakwa menjadi kurir narkoba lintas kota untuk mengedarkan sabu atas perintah seseorang bernama Ek...\",\n",
    "      \"predicted\": [\n",
    "        8,\n",
    "        33,\n",
    "        34,\n",
    "        23,\n",
    "        11\n",
    "      ],\n",
    "      \"ground_truth\": [\n",
    "        4,\n",
    "        5,\n",
    "        6\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"query_id\": 5,\n",
    "      \"query\": \"mahasiswa ditangkap setelah terbukti menjual tembakau sintetis via media sosial dengan bukti chat pe...\",\n",
    "      \"predicted\": [\n",
    "        24,\n",
    "        30,\n",
    "        14,\n",
    "        13,\n",
    "        12\n",
    "      ],\n",
    "      \"ground_truth\": [\n",
    "        5,\n",
    "        6,\n",
    "        7\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"query_id\": 9,\n",
    "      \"query\": \"terdakwa merupakan bagian dari jaringan internasional yang menyuplai sabu ke beberapa kota besar di ...\",\n",
    "      \"predicted\": [\n",
    "        8,\n",
    "        19,\n",
    "        23,\n",
    "        29,\n",
    "        4\n",
    "      ],\n",
    "      \"ground_truth\": [\n",
    "        9,\n",
    "        10,\n",
    "        11\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "\n",
    "\n",
    "- **Kesalahan pada TF-IDF + SVM:**\n",
    "Disimpan di: `data/eval/error_cases_svm.json`\n",
    "  ```json\n",
    "[\n",
    "  {\n",
    "    \"query_id\": 2,\n",
    "    \"query\": \"pada saat dilakukan penggeledahan terhadap terdakwa ditemukan 1 bungkus plastik bening narkotika jen...\",\n",
    "    \"predicted\": [\n",
    "      8\n",
    "    ],\n",
    "    \"ground_truth\": [\n",
    "      2,\n",
    "      3,\n",
    "      4\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"query_id\": 3,\n",
    "    \"query\": \"terdakwa mengakui menyimpan ekstasi sebanyak 50 butir di lemari rumahnya setelah mendapatkannya dari...\",\n",
    "    \"predicted\": [\n",
    "      11\n",
    "    ],\n",
    "    \"ground_truth\": [\n",
    "      3,\n",
    "      4,\n",
    "      5\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"query_id\": 4,\n",
    "    \"query\": \"terdakwa menjadi kurir narkoba lintas kota untuk mengedarkan sabu atas perintah seseorang bernama Ek...\",\n",
    "    \"predicted\": [\n",
    "      8\n",
    "    ],\n",
    "    \"ground_truth\": [\n",
    "      4,\n",
    "      5,\n",
    "      6\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"query_id\": 5,\n",
    "    \"query\": \"mahasiswa ditangkap setelah terbukti menjual tembakau sintetis via media sosial dengan bukti chat pe...\",\n",
    "    \"predicted\": [\n",
    "      24\n",
    "    ],\n",
    "    \"ground_truth\": [\n",
    "      5,\n",
    "      6,\n",
    "      7\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"query_id\": 8,\n",
    "    \"query\": \"pelajar berperan menjadi perantara jual beli narkoba jenis sabu dengan upah 500 ribu rupiah per gram...\",\n",
    "    \"predicted\": [\n",
    "      1\n",
    "    ],\n",
    "    \"ground_truth\": [\n",
    "      8,\n",
    "      9,\n",
    "      10\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"query_id\": 9,\n",
    "    \"query\": \"terdakwa merupakan bagian dari jaringan internasional yang menyuplai sabu ke beberapa kota besar di ...\",\n",
    "    \"predicted\": [\n",
    "      19\n",
    "    ],\n",
    "    \"ground_truth\": [\n",
    "      9,\n",
    "      10,\n",
    "      11\n",
    "    ]\n",
    "  }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "63915465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUKSES] Tahap 5 - Evaluasi Model selesai\n",
      "- Hasil evaluasi retrieval disimpan di: data/eval/retrieval_metrics.csv\n",
      "- Hasil evaluasi prediksi disimpan di : data/eval/prediction_metrics.csv\n",
      "- Visualisasi disimpan di: data/eval/performance_comparison.png\n",
      "- Error cosine disimpan di: data/eval/error_cases_cosine.json\n",
      "- Error svm disimpan di: data/eval/error_cases_svm.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from typing import List, Dict\n",
    "\n",
    "# === PATH ===\n",
    "COSINE_PRED = \"data/results/predictions_cosine.csv\"\n",
    "SVM_PRED = \"data/results/predictions_svm.csv\"\n",
    "EVAL_QUERIES = \"data/eval/queries.json\"\n",
    "RETRIEVAL_METRIC_OUTPUT = \"data/eval/retrieval_metrics.csv\"\n",
    "PREDICTION_METRIC_OUTPUT = \"data/eval/prediction_metrics.csv\"\n",
    "ERROR_COSINE_OUTPUT = \"data/eval/error_cases_cosine.json\"\n",
    "ERROR_SVM_OUTPUT = \"data/eval/error_cases_svm.json\"\n",
    "os.makedirs(\"data/eval\", exist_ok=True)\n",
    "\n",
    "# === LOAD GROUND TRUTH ===\n",
    "with open(EVAL_QUERIES, encoding='utf-8') as f:\n",
    "    ground_truth_data = json.load(f)\n",
    "    gt_dict = {i+1: item[\"ground_truth\"] for i, item in enumerate(ground_truth_data)}\n",
    "\n",
    "# === EVALUASI RETRIEVAL (COSINE: TOP-K MATCHING) ===\n",
    "def eval_retrieval(pred_file: str, model_name: str, k: int = 5) -> Dict:\n",
    "    df = pd.read_csv(pred_file)\n",
    "    y_true, y_pred = [], []\n",
    "    for _, row in df.iterrows():\n",
    "        query_id = int(row[\"query_id\"])\n",
    "        pred_ids = eval(row[\"top_5_case_ids\"])[:k]\n",
    "        gt = gt_dict[query_id]\n",
    "        hit = any(pid in gt for pid in pred_ids)\n",
    "        y_true.append(1)\n",
    "        y_pred.append(1 if hit else 0)\n",
    "\n",
    "    return {\n",
    "        \"model\": model_name,\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"recall\":    recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"f1_score\":  f1_score(y_true, y_pred, zero_division=0)\n",
    "    }\n",
    "\n",
    "# === EVALUASI PREDIKSI SVM SEBAGAI RETRIEVAL (TOP-1 MATCH) ===\n",
    "def eval_prediction(pred_file: str, model_name: str) -> Dict:\n",
    "    df = pd.read_csv(pred_file)\n",
    "    y_true, y_pred = [], []\n",
    "    for _, row in df.iterrows():\n",
    "        query_id = int(row[\"query_id\"])\n",
    "        pred_ids = eval(row[\"top_5_case_ids\"])\n",
    "        pred = pred_ids[0] if pred_ids else -1\n",
    "        gt = gt_dict[query_id]\n",
    "        hit = pred in gt\n",
    "        y_true.append(1)\n",
    "        y_pred.append(1 if hit else 0)\n",
    "\n",
    "    return {\n",
    "        \"model\": model_name,\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"recall\":    recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"f1_score\":  f1_score(y_true, y_pred, zero_division=0)\n",
    "    }\n",
    "\n",
    "# === SIMPAN KASUS GAGAL (ERROR ANALYSIS) ===\n",
    "def save_errors(pred_file: str, output_file: str):\n",
    "    df = pd.read_csv(pred_file)\n",
    "    error_cases = []\n",
    "    for _, row in df.iterrows():\n",
    "        query_id = int(row[\"query_id\"])\n",
    "        pred_ids = eval(row[\"top_5_case_ids\"])\n",
    "        gt = gt_dict[query_id]\n",
    "        if not any(pid in gt for pid in pred_ids):\n",
    "            error_cases.append({\n",
    "                \"query_id\": query_id,\n",
    "                \"query\": row[\"query\"],\n",
    "                \"predicted\": pred_ids,\n",
    "                \"ground_truth\": gt\n",
    "            })\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(error_cases, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# === JALANKAN EVALUASI ===\n",
    "retrieval_metrics  = eval_retrieval(COSINE_PRED, \"TF-IDF + Cosine\")\n",
    "prediction_metrics = eval_prediction(SVM_PRED,   \"TF-IDF + SVM\")\n",
    "\n",
    "# === SIMPAN METRIK KE FILE TERPISAH ===\n",
    "pd.DataFrame([retrieval_metrics]).to_csv(RETRIEVAL_METRIC_OUTPUT, index=False)\n",
    "pd.DataFrame([prediction_metrics]).to_csv(PREDICTION_METRIC_OUTPUT, index=False)\n",
    "\n",
    "# === SIMPAN KASUS GAGAL ===\n",
    "save_errors(COSINE_PRED, ERROR_COSINE_OUTPUT)\n",
    "save_errors(SVM_PRED,    ERROR_SVM_OUTPUT)\n",
    "\n",
    "# === VISUALISASI ===\n",
    "combined_df = pd.DataFrame([retrieval_metrics, prediction_metrics])\n",
    "ax = combined_df.set_index(\"model\")[[\"accuracy\", \"precision\", \"recall\", \"f1_score\"]].plot(\n",
    "    kind=\"bar\", figsize=(10, 6), title=\"Perbandingan Model\", ylim=(0, 1)\n",
    ")\n",
    "\n",
    "# -- Tambah angka skor di atas setiap batang --\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fmt='%.2f', label_type='edge')\n",
    "\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"data/eval/performance_comparison.png\")\n",
    "plt.close()\n",
    "\n",
    "print(\"[SUKSES] Tahap 5 - Evaluasi Model selesai\")\n",
    "print(f\"- Hasil evaluasi retrieval disimpan di: {RETRIEVAL_METRIC_OUTPUT}\")\n",
    "print(f\"- Hasil evaluasi prediksi disimpan di : {PREDICTION_METRIC_OUTPUT}\")\n",
    "print(\"- Visualisasi disimpan di: data/eval/performance_comparison.png\")\n",
    "print(f\"- Error cosine disimpan di: {ERROR_COSINE_OUTPUT}\")\n",
    "print(f\"- Error svm disimpan di: {ERROR_SVM_OUTPUT}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4db16c5",
   "metadata": {},
   "source": [
    "### Kesimpulan\n",
    "\n",
    "Proyek ini sukses membangun sebuah sistem **Case‑Based Reasoning (CBR)** berbasis **representasi teks TF‑IDF** untuk menganalisis putusan pengadilan. Pengerjaan berlangsung melalui lima langkah utama:\n",
    "\n",
    "1. **Penyusunan Case Base**\n",
    "   Pada tahap awal, sistem membangun *case base* dengan melakukan *web scraping* terhadap 35 dokumen putusan pidana dari situs resmi Direktori Putusan Mahkamah Agung. Seluruh dokumen yang diperoleh kemudian dibersihkan dari noise, divalidasi untuk memastikan kelengkapan isi, dan disimpan dalam format `.txt` agar mudah diolah pada tahap selanjutnya. Tahap ini memastikan bahwa data sumber berkualitas baik dan siap digunakan sebagai basis pembelajaran dan referensi.\n",
    "\n",
    "2. **Representasi Kasus**\n",
    "   Setelah data dikumpulkan, setiap kasus direpresentasikan dalam format terstruktur yang mencakup informasi penting seperti metadata (nomor, jenis perkara, tanggal), ringkasan fakta hukum, pasal-pasal yang dikenakan, identitas pihak, serta isi putusan lengkap. Selain itu, proses *feature engineering* dilakukan untuk mengekstrak fitur penting dari dokumen, seperti panjang teks, tokenisasi berbasis Bag-of-Words, dan penyusunan pasangan tanya-jawab (QA-pairs) yang mendukung analisis semantik.\n",
    "\n",
    "3. **Case Retrieval**\n",
    "   Untuk menemukan kasus serupa, sistem mengimplementasikan dua pendekatan retrieval berbasis teks. Pertama, pendekatan **TF-IDF + Cosine Similarity** digunakan untuk menghitung kemiripan antara vektor representasi kasus baru dan kasus lama. Kedua, pendekatan klasifikasi terawasi menggunakan **TF-IDF + LinearSVC (SVM)** dikembangkan untuk memprediksi ID kasus yang paling mendekati. Kedua metode ini diuji dan dibandingkan untuk melihat mana yang lebih efektif dalam domain hukum.\n",
    "\n",
    "4. **Solution Reuse**\n",
    "   Tahap ini bertujuan untuk menghasilkan solusi bagi kasus baru dengan cara memanfaatkan solusi dari kasus lama yang paling relevan. Fungsi `predict_outcome` diimplementasikan untuk mengambil *amar putusan* dari kasus hasil retrieval teratas (top‑1), lalu menyajikannya dalam bentuk ringkasan ringkas sebagai prediksi terhadap putusan kasus baru. Pendekatan ini sejalan dengan prinsip reasoning berbasis analogi dalam CBR.\n",
    "\n",
    "5. **Evaluasi Model**\n",
    "   Evaluasi dilakukan untuk mengukur performa sistem secara kuantitatif menggunakan metrik klasifikasi: Accuracy, Precision, Recall, dan F1-score. Hasil evaluasi menunjukkan bahwa pendekatan TF-IDF + Cosine memiliki kinerja yang lebih stabil dan presisi tinggi pada konteks retrieval kasus, sementara model TF-IDF + SVM masih memiliki keterbatasan dalam menghadapi variasi data kasus baru. Hal ini mengindikasikan bahwa pendekatan berbasis kemiripan teks lebih cocok untuk domain yurisprudensi dengan data terbatas.\n",
    "\n",
    "---\n",
    "\n",
    "| Pendekatan      | Accuracy | Precision | Recall | F1‑score |\n",
    "| --------------- | -------- | --------- | ------ | -------- |\n",
    "| TF‑IDF + Cosine | 0.60     | 1.00      | 0.60   | 0.75     |\n",
    "| TF‑IDF + SVM    | 0.40     | 1.00      | 0.40   | 0.57     |\n",
    "\n",
    "Visualisasi performa dan analisis *error* mempertegas bahwa metode berbasis kemiripan vektor lebih cocok untuk domain CBR ketika data pelatihan terbatas atau tidak seimbang.\n",
    "\n",
    "---\n",
    "\n",
    "### Penutup\n",
    "\n",
    "penelitian ini menunjukkan bahwa kombinasi representasi teks dan algoritme retrieval mampu memberikan solusi praktis bagi analisis yurisprudensi secara komputasional. Ke depan, sistem dapat diperluas dengan embedding modern (mis. BERT) dan diintegrasikan ke basis data yudisial nasional guna meningkatkan cakupan dan akurasi.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
