{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc311b49",
   "metadata": {},
   "source": [
    "---\n",
    "# Perbandingan Pendekatan CBR Berbasis TF-IDF: Cosine Similarity vs. SVM dalam Analisis Putusan Pengadilan\n",
    "\n",
    "Anggota Kelompok :\n",
    "\n",
    "1. Muhamad Ahdan Fauzan - 202210370311456\n",
    "\n",
    "2. Khairy - 202210370311439\n",
    "\n",
    "## Tugas Besar Mata Kuliah Penalaran Komputer (A)\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b548b8a",
   "metadata": {},
   "source": [
    "# Tahap 1 – Membangun Case Base\n",
    "\n",
    "## Tujuan\n",
    "\n",
    "Tahap ini bertujuan untuk membangun basis kasus (case base) awal dengan cara mengumpulkan, mengekstrak, dan membersihkan dokumen putusan pengadilan dari sumber resmi Mahkamah Agung Republik Indonesia. Hasil akhir dari tahap ini berupa teks putusan yang telah dibersihkan dan siap diproses lebih lanjut dalam siklus Case-Based Reasoning (CBR).\n",
    "\n",
    "---\n",
    "\n",
    "## Langkah Kerja\n",
    "\n",
    "### 1. Seleksi dan Pengunduhan Dokumen\n",
    "\n",
    "- Domain perkara yang dipilih: **Pidana khusus – Narkotika dan Psikotropika (PN Bandung)**\n",
    "- Sumber dokumen: Direktori Putusan Mahkamah Agung Republik Indonesia\n",
    "- Format dokumen: PDF\n",
    "- Jumlah dokumen: **35 dokumen**\n",
    "\n",
    "Dokumen diunduh secara manual dan disimpan dalam folder `pdf_downloaded/`.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Konversi dan Ekstraksi Teks\n",
    "\n",
    "Setiap file PDF dikonversi menjadi teks polos (plain text) menggunakan pustaka `pdfminer`. Tujuan dari konversi ini adalah untuk memperoleh isi putusan dalam format yang bisa diproses lebih lanjut.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Pembersihan Teks (Cleaning)\n",
    "\n",
    "Teks hasil ekstraksi dibersihkan dengan cara:\n",
    "\n",
    "- Menghapus watermark, header, footer, dan nomor halaman\n",
    "- Menghapus konten disclaimer dari MA RI\n",
    "- Menormalkan spasi dan huruf (lowercase)\n",
    "- Menghitung rasio keutuhan dokumen (panjang teks bersih dibanding teks awal)\n",
    "\n",
    "Dokumen hanya disimpan jika memenuhi syarat minimal rasio keutuhan ≥ 80%.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Validasi dan Logging\n",
    "\n",
    "Semua dokumen dicatat dalam log proses pembersihan (`logs/cleaning.log`) dengan informasi rasio keutuhan per kasus. Log ini berguna untuk memantau kelayakan data dan mendeteksi dokumen bermasalah.\n",
    "\n",
    "---\n",
    "\n",
    "## Output Tahap Ini\n",
    "\n",
    "- Folder `/data/raw/*.txt` berisi 47 file teks putusan yang telah dibersihkan dan lolos validasi.\n",
    "- File log `/logs/cleaning.log` berisi rekaman validasi keutuhan untuk setiap kasus.\n",
    "- Semua dokumen memiliki rasio keutuhan di atas 88%, menandakan proses ekstraksi dan cleaning berhasil dilakukan dengan baik.\n",
    "\n",
    "Contoh log validasi:\n",
    "[OK] case_001 diproses (89.08% valid)\n",
    "[OK] case_002 diproses (89.29% valid)\n",
    "[OK] case_003 diproses (89.45% valid)\n",
    "...\n",
    "[OK] case_047 diproses (89.34% valid)\n",
    "\n",
    "\n",
    "---\n",
    "Tahap pertama ini berhasil menyiapkan kumpulan kasus dengan kualitas teks yang layak untuk digunakan sebagai basis kasus pada sistem CBR. Tahapan ini menjadi fondasi penting untuk proses representasi dan retrieval pada tahap-tahap selanjutnya.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "749746d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] case_001 diproses (97.42% valid).\n",
      "[OK] case_002 diproses (97.03% valid).\n",
      "[OK] case_003 diproses (97.05% valid).\n",
      "[OK] case_004 diproses (96.98% valid).\n",
      "[OK] case_005 diproses (96.94% valid).\n",
      "[OK] case_006 diproses (96.92% valid).\n",
      "[OK] case_007 diproses (96.92% valid).\n",
      "[OK] case_008 diproses (97.13% valid).\n",
      "[OK] case_009 diproses (96.93% valid).\n",
      "[OK] case_010 diproses (96.93% valid).\n",
      "[OK] case_011 diproses (97.03% valid).\n",
      "[OK] case_012 diproses (97.04% valid).\n",
      "[OK] case_013 diproses (97.09% valid).\n",
      "[OK] case_014 diproses (97.09% valid).\n",
      "[OK] case_015 diproses (97.03% valid).\n",
      "[OK] case_016 diproses (96.96% valid).\n",
      "[OK] case_017 diproses (96.85% valid).\n",
      "[OK] case_018 diproses (96.84% valid).\n",
      "[OK] case_019 diproses (96.97% valid).\n",
      "[OK] case_020 diproses (97.04% valid).\n",
      "[OK] case_021 diproses (97.01% valid).\n",
      "[OK] case_022 diproses (97.02% valid).\n",
      "[OK] case_023 diproses (96.96% valid).\n",
      "[OK] case_024 diproses (97.12% valid).\n",
      "[OK] case_025 diproses (96.99% valid).\n",
      "[OK] case_026 diproses (97.38% valid).\n",
      "[OK] case_027 diproses (96.83% valid).\n",
      "[OK] case_028 diproses (96.95% valid).\n",
      "[OK] case_029 diproses (96.88% valid).\n",
      "[OK] case_030 diproses (97.26% valid).\n",
      "[OK] case_031 diproses (96.82% valid).\n",
      "[OK] case_032 diproses (96.95% valid).\n",
      "[OK] case_033 diproses (97.06% valid).\n",
      "[OK] case_034 diproses (97.10% valid).\n",
      "[OK] case_035 diproses (96.77% valid).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import string  # ← tambahkan ini\n",
    "from pdfminer.high_level import extract_text\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# === Konfigurasi path ===\n",
    "PDF_FOLDER = 'pdf_downloaded'\n",
    "OUTPUT_FOLDER = 'data/raw'\n",
    "LOG_FILE = 'logs/cleaning.log'\n",
    "\n",
    "def clean_text(text):\n",
    "\n",
    "    baseline_length = len(text)\n",
    "   \n",
    "    # Hapus header/footer watermark\n",
    "    text = text.replace(\"Direktori Putusan Mahkamah Agung Republik Indonesia\", \"\")\n",
    "    text = text.replace(\"putusan.mahkamahagung.go.id\", \"\")\n",
    "    text = re.sub(r'halaman\\s*\\d+', '', text, flags=re.IGNORECASE)\n",
    "    text = text.replace(\"M a h ka m a h A g u n g R e p u blik In d o n esia\\n\", \"\")\n",
    "    cleaned_length = len(text)\n",
    "    text = text.replace(\"Disclaimer\\n\", \"\")\n",
    "    text = text.replace(\n",
    "        \"Kepaniteraan Mahkamah Agung Republik Indonesia berusaha untuk selalu mencantumkan informasi paling kini dan akurat sebagai bentuk komitmen Mahkamah Agung untuk pelayanan publik, transparansi dan akuntabilitas\\n\", \"\")\n",
    "    text = text.replace(\n",
    "        \"pelaksanaan fungsi peradilan. Namun dalam hal-hal tertentu masih dimungkinkan terjadi permasalahan teknis terkait dengan akurasi dan keterkinian informasi yang kami sajikan, hal mana akan terus kami perbaiki dari waktu kewaktu.\\n\", \"\")\n",
    "    text = text.replace(\n",
    "        \"Dalam hal Anda menemukan inakurasi informasi yang termuat pada situs ini atau informasi yang seharusnya ada, namun belum tersedia, maka harap segera hubungi Kepaniteraan Mahkamah Agung RI melalui :\\n\", \"\")\n",
    "    text = text.replace(\n",
    "        \"Email : kepaniteraan@mahkamahagung.go.id    Telp : 021-384 3348 (ext.318)\\n\", \"\")\n",
    "    \n",
    "\n",
    "\n",
    "    # Normalisasi akhir\n",
    "    text = text.lower()\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    ratio = cleaned_length / baseline_length \n",
    "\n",
    "    return text, ratio\n",
    "\n",
    "\n",
    "\n",
    "# === Fungsi log (opsional) ===\n",
    "def write_log(case_name, ratio):\n",
    "    os.makedirs(os.path.dirname(LOG_FILE), exist_ok=True)\n",
    "    with open(LOG_FILE, 'a', encoding='utf-8') as f:\n",
    "        timestamp = datetime.now().isoformat()\n",
    "        f.write(f\"[{timestamp}] {case_name} | Integrity: {ratio:.2%}\\n\")\n",
    "\n",
    "# === Proses utama ===\n",
    "def process_pdfs():\n",
    "    os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "    pdf_files = sorted(glob.glob(os.path.join(PDF_FOLDER, '*.pdf')))\n",
    "    \n",
    "    for i, pdf_path in enumerate(pdf_files):\n",
    "        case_id = f\"case_{i+1:03d}\"\n",
    "        output_file = os.path.join(OUTPUT_FOLDER, f\"{case_id}.txt\")\n",
    "\n",
    "        try:\n",
    "            # Ekstrak teks\n",
    "            text = extract_text(pdf_path)\n",
    "\n",
    "            # Bersihkan\n",
    "            cleaned_text, ratio = clean_text(text)\n",
    "\n",
    "            # Validasi keutuhan\n",
    "            if ratio < 0.8:\n",
    "                print(f\"[WARNING] {case_id} hanya {ratio:.2%} isi yang tersisa.\")\n",
    "            else:\n",
    "                print(f\"[OK] {case_id} diproses ({ratio:.2%} valid).\")\n",
    "\n",
    "                # Simpan teks bersih HANYA jika valid\n",
    "                with open(output_file, 'w', encoding='utf-8') as f_out:\n",
    "                    f_out.write(cleaned_text)\n",
    "\n",
    "            # Catat log tetap dicatat semuanya\n",
    "            write_log(case_id, ratio)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Gagal memproses {pdf_path}: {e}\")\n",
    "\n",
    "# === Eksekusi utama ===\n",
    "if __name__ == \"__main__\":\n",
    "    process_pdfs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cdcccf",
   "metadata": {},
   "source": [
    "# Tahap 2 – Case Representation\n",
    "\n",
    "## Tujuan\n",
    "\n",
    "Tahapan ini bertujuan untuk merepresentasikan setiap putusan dalam struktur data yang terorganisir. Hasil representasi ini menjadi basis data terstruktur yang siap digunakan untuk proses retrieval dan analisis lebih lanjut dalam sistem Case-Based Reasoning (CBR).\n",
    "\n",
    "---\n",
    "\n",
    "## Langkah Kerja\n",
    "\n",
    "### 1. Ekstraksi Metadata\n",
    "\n",
    "Setiap dokumen hasil cleaning dianalisis untuk mengekstrak informasi penting sebagai metadata, meliputi:\n",
    "\n",
    "- Nomor Perkara (`no_perkara`)\n",
    "- Tanggal Putusan (`tanggal`)\n",
    "- Ringkasan Fakta (`ringkasan_fakta`)\n",
    "- Pasal yang didakwakan (`pasal`)\n",
    "- Pihak terkait (Terdakwa dan Korban)\n",
    "- Isi teks lengkap (`text_full`)\n",
    "\n",
    "Ekstraksi dilakukan dengan pendekatan berbasis pola (regex) terhadap isi dokumen.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Penyimpanan Data Terstruktur\n",
    "\n",
    "Data hasil ekstraksi disimpan dalam dua format:\n",
    "\n",
    "- **CSV**: `data/processed/cases_extracted.csv`\n",
    "- **JSON**: `data/processed/cases_extracted.json`\n",
    "\n",
    "Struktur kolom yang digunakan meliputi:\n",
    "\n",
    "- `case_id`\n",
    "- `no_perkara`\n",
    "- `tanggal`\n",
    "- `ringkasan_fakta`\n",
    "- `pasal`\n",
    "- `pihak`\n",
    "- `text_full`\n",
    "\n",
    "Jumlah data yang berhasil diproses: **35 kasus**\n",
    "\n",
    "Contoh output terminal:\n",
    "\n",
    "[SUKSES] 35 kasus disimpan ke:\n",
    "\n",
    "CSV → data/processed/cases_extracted.csv\n",
    "\n",
    "JSON → data/processed/cases_extracted.json\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Feature Engineering\n",
    "\n",
    "Untuk meningkatkan pemanfaatan data kasus, dilakukan proses rekayasa fitur (feature engineering) yang meliputi:\n",
    "\n",
    "- **Jumlah Kata (Length)**: Menghitung total token (kata) dalam teks.\n",
    "- **Bag-of-Words (BoW)**: Menghitung frekuensi kata dalam setiap kasus.\n",
    "- **QA-Pairs Sederhana**: Menghasilkan pasangan pertanyaan dan jawaban dari konten teks.\n",
    "\n",
    "QA-Pairs mencakup contoh pertanyaan berikut:\n",
    "\n",
    "- Apa nomor perkaranya?\n",
    "- Apa pasal yang dilanggar?\n",
    "- Siapa terdakwanya?\n",
    "- Siapa korbannya?\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Penyimpanan Fitur\n",
    "\n",
    "Hasil rekayasa fitur disimpan dalam format JSON:\n",
    "\n",
    "- `data/processed/features_length.json`\n",
    "- `data/processed/features_bow.json`\n",
    "- `data/processed/features_qa_pairs.json`\n",
    "\n",
    "Contoh output terminal:\n",
    "\n",
    "[SUKSES] Feature Engineering selesai!\n",
    "\n",
    "Length disimpan di : data/processed/features_length.json\n",
    "\n",
    "Bag-of-Words disimpan di: data/processed/features_bow.json\n",
    "\n",
    "QA-pairs disimpan di : data/processed/features_qa_pairs.json\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Tahap representasi berhasil membentuk struktur data terorganisir untuk 35 kasus. Setiap kasus dilengkapi metadata, ringkasan fakta, dan fitur tambahan untuk mendukung proses retrieval dan prediksi pada tahapan selanjutnya dalam sistem CBR.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bcfbd997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUKSES] 35 kasus disimpan ke:\n",
      "- CSV  → data/processed/cases_extracted.csv\n",
      "- JSON → data/processed/cases_extracted.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# === Path Konfigurasi ===\n",
    "RAW_FOLDER  = 'data/raw'\n",
    "CSV_OUTPUT  = 'data/processed/cases_extracted.csv'\n",
    "JSON_OUTPUT = 'data/processed/cases_extracted.json'\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "\n",
    "# === Fungsi pencarian hasil pertama ===\n",
    "def find_first(pattern, text):\n",
    "    hits = re.findall(pattern, text, re.IGNORECASE | re.DOTALL)\n",
    "    return hits[0].strip() if hits else \"\"\n",
    "\n",
    "# === Fungsi membersihkan nilai CSV ===\n",
    "def clean_for_csv(text):\n",
    "    return \"\" if not text else text.replace(\",\", \" \").replace(\"\\n\", \" \").strip()\n",
    "\n",
    "# === Ekstraksi data per dokumen ===\n",
    "def extract_case_data(case_id, text):\n",
    "    text_clean = \" \".join(text.split())\n",
    "\n",
    "    no_perkara = clean_for_csv(find_first(r\"p\\s*u\\s*t\\s*u\\s*s\\s*a\\s*n\\s*\\\"?(.*?)\\\"?\\s*demi keadilan\", text_clean))\n",
    "    tanggal    = find_first(r'pada hari\\s+\\w+\\s+tanggal\\s+(\\d{1,2}\\s+\\w+\\s+\\d{4})', text_clean)\n",
    "\n",
    "    # ---------- Ringkasan fakta: 4 opsi ----------\n",
    "    patterns_fakta = [\n",
    "        r'ditemukan\\s+barang\\s+bukti\\s+berupa\\s*(.*?)(?=\\n\\s*\\n|menimbang)',                 # opsi 1\n",
    "        r'bahwa\\s+benar\\s+barang\\s+bukti\\s*(.*?)(?=\\n\\s*\\n|menimbang)',                     # opsi 2\n",
    "        r'barang\\s+bukti\\s+berupa\\s*(.*?)(?=\\n\\s*\\n|menimbang)',                            # opsi 3\n",
    "        r'bahwa\\s+terhadap\\s+barang\\s+bukti\\s+dalam\\s+perkara\\s*(.*?)(?=\\n\\s*\\n|menimbang)' # opsi 4\n",
    "    ]\n",
    "    ringkasan_fakta = \"\"\n",
    "    for pat in patterns_fakta:\n",
    "        ringkasan_fakta = find_first(pat, text_clean)\n",
    "        if ringkasan_fakta:\n",
    "            break\n",
    "\n",
    "    pasal = find_first(r'(pasal.+?narkotika)', text_clean)\n",
    "\n",
    "    # --- Pihak terdakwa / saksi ---\n",
    "    m_mengadili = re.search(r'mengadili\\s*:\\s*1\\.', text_clean, re.IGNORECASE)\n",
    "    after_mengadili = text_clean[m_mengadili.end():] if m_mengadili else text_clean\n",
    "    pihak_terdakwa  = find_first(\n",
    "        r'menyatakan(?: bahwa)?(?: ia)?\\s+(?:terdakwa|saksi)\\s+(.+?)\\s+(?:terbukti|telah|tersebut)',\n",
    "        after_mengadili\n",
    "    )\n",
    "    korban_match = find_first(r'korban\\s+([a-zA-Z]+)', text_clean)\n",
    "    pihak = f\"Terdakwa: {pihak_terdakwa} Korban: {korban_match}\" if (pihak_terdakwa or korban_match) else \"\"\n",
    "\n",
    "    return {\n",
    "        \"case_id\":        case_id,\n",
    "        \"no_perkara\":     clean_for_csv(no_perkara),\n",
    "        \"tanggal\":        clean_for_csv(tanggal),\n",
    "        \"ringkasan_fakta\":clean_for_csv(ringkasan_fakta),\n",
    "        \"pasal\":          clean_for_csv(pasal),\n",
    "        \"pihak\":          clean_for_csv(pihak),\n",
    "        \"text_full\":      clean_for_csv(text)\n",
    "    }\n",
    "\n",
    "# === Proses semua dokumen ===\n",
    "cases = []\n",
    "for i in range(1, 48):                      # case_001.txt – case_047.txt\n",
    "    fp = os.path.join(RAW_FOLDER, f\"case_{i:03}.txt\")\n",
    "    if not os.path.exists(fp):\n",
    "        continue\n",
    "    with open(fp, encoding='utf-8') as f:\n",
    "        cases.append(extract_case_data(i, f.read()))\n",
    "\n",
    "# === Simpan hasil ===\n",
    "pd.DataFrame(cases).to_csv(CSV_OUTPUT, index=False)\n",
    "with open(JSON_OUTPUT, 'w', encoding='utf-8') as jf:\n",
    "    json.dump(cases, jf, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"[SUKSES] {len(cases)} kasus disimpan ke:\")\n",
    "print(f\"- CSV  → {CSV_OUTPUT}\")\n",
    "print(f\"- JSON → {JSON_OUTPUT}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f2b4bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUKSES] Feature Engineering selesai!\n",
      "- Length disimpan di      : data/processed\\features_length.json\n",
      "- Bag-of-Words disimpan di: data/processed\\features_bow.json\n",
      "- QA-pairs disimpan di    : data/processed\\features_qa_pairs.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# === Path ===\n",
    "RAW_FOLDER = 'data/raw'\n",
    "PROCESSED_FOLDER = 'data/processed'\n",
    "os.makedirs(PROCESSED_FOLDER, exist_ok=True)\n",
    "\n",
    "# === File Output Feature Engineering ===\n",
    "LENGTH_FILE = os.path.join(PROCESSED_FOLDER, 'features_length.json')\n",
    "BOW_FILE = os.path.join(PROCESSED_FOLDER, 'features_bow.json')\n",
    "QA_FILE = os.path.join(PROCESSED_FOLDER, 'features_qa_pairs.json')\n",
    "\n",
    "# === Tokenizer sederhana ===\n",
    "def tokenize(text):\n",
    "    return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "# === Ekstraksi QA pairs ===\n",
    "def get_qa_pairs(text):\n",
    "    return {\n",
    "        \"Apa nomor perkaranya?\": re.search(r\"putusan\\s+\\\"?([^\\\"\\,\\n]+)\", text, re.IGNORECASE | re.DOTALL),\n",
    "        \"Apa pasal yang dilanggar?\": re.search(r\"melanggar pasal\\s+\\\"?([^\\\"\\,\\.\\;\\:]+)\", text, re.IGNORECASE),\n",
    "        \"Siapa terdakwanya?\": re.search(r\"terdakwa\\s+([a-zA-Z]+)\", text, re.IGNORECASE),\n",
    "        \"Siapa korbannya?\": re.search(r\"korban\\s+([a-zA-Z]+)\", text, re.IGNORECASE),\n",
    "    }\n",
    "\n",
    "# === Proses semua file ===\n",
    "length_data = {}\n",
    "bow_data = {}\n",
    "qa_data = {}\n",
    "\n",
    "for i in range(1, 48):  # case_001.txt - case_047.txt\n",
    "    file_path = os.path.join(RAW_FOLDER, f\"case_{i:03}.txt\")\n",
    "    if not os.path.exists(file_path):\n",
    "        continue\n",
    "\n",
    "    with open(file_path, encoding='utf-8') as f:\n",
    "        raw_text = f.read()\n",
    "\n",
    "    text_clean = \" \".join(raw_text.split())\n",
    "    tokens = tokenize(text_clean)\n",
    "    case_id = f\"case_{i:03}\"\n",
    "\n",
    "    # Jumlah kata\n",
    "    length_data[case_id] = len(tokens)\n",
    "\n",
    "    # Bag-of-Words\n",
    "    bow_data[case_id] = dict(Counter(tokens))\n",
    "\n",
    "    # QA Pairs\n",
    "    qas = get_qa_pairs(text_clean)\n",
    "    qa_data[case_id] = {q: m.group(1).strip() if m else None for q, m in qas.items()}\n",
    "\n",
    "# === Simpan ke file JSON terpisah ===\n",
    "with open(LENGTH_FILE, 'w', encoding='utf-8') as f:\n",
    "    json.dump(length_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(BOW_FILE, 'w', encoding='utf-8') as f:\n",
    "    json.dump(bow_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(QA_FILE, 'w', encoding='utf-8') as f:\n",
    "    json.dump(qa_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"[SUKSES] Feature Engineering selesai!\")\n",
    "print(f\"- Length disimpan di      : {LENGTH_FILE}\")\n",
    "print(f\"- Bag-of-Words disimpan di: {BOW_FILE}\")\n",
    "print(f\"- QA-pairs disimpan di    : {QA_FILE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131ef579",
   "metadata": {},
   "source": [
    "# Tahap 3 – Case Retrieval\n",
    "\n",
    "## Tujuan\n",
    "\n",
    "Tahap ini bertujuan untuk menemukan kasus-kasus lama yang paling relevan dan mirip dengan query kasus baru yang diajukan. Proses ini merupakan bagian utama dalam sistem Case-Based Reasoning (CBR) untuk mendukung analisis dan pencarian preseden hukum.\n",
    "\n",
    "---\n",
    "\n",
    "## Langkah Kerja\n",
    "\n",
    "### 1. Representasi Vektor\n",
    "\n",
    "- Setiap ringkasan fakta dari putusan diubah menjadi representasi vektor menggunakan algoritma **TF-IDF** (`TfidfVectorizer` dari `sklearn`).\n",
    "- Alternatif lain yang tersedia namun tidak digunakan pada tahap ini adalah embedding berbasis **transformer** seperti **IndoBERT**.\n",
    "\n",
    "### 2. Splitting Data\n",
    "\n",
    "- Dataset dibagi menjadi dua bagian: **data latih (train)** dan **data uji (test)** dengan rasio **80:20**.\n",
    "- Teknik ini digunakan untuk pelatihan model klasifikasi berbasis TF-IDF + SVM.\n",
    "\n",
    "### 3. Model Retrieval\n",
    "\n",
    "Dalam tahap ini, sistem dibangun menggunakan **dua pendekatan berbeda** untuk melakukan retrieval terhadap kasus lama yang paling relevan dengan query baru:\n",
    "\n",
    "#### a. TF-IDF + Cosine Similarity (Pendekatan Case-Based Reasoning)\n",
    "\n",
    "- Menggunakan **TF-IDF vectorizer** untuk merepresentasikan teks ringkasan fakta dari semua kasus sebagai vektor numerik.\n",
    "- Query kasus baru juga diubah menjadi vektor menggunakan TF-IDF yang sama.\n",
    "- Kemiripan antar vektor dihitung menggunakan **cosine similarity**.\n",
    "- Top-k kasus dengan skor kemiripan tertinggi dipilih sebagai hasil retrieval.\n",
    "- Pendekatan ini bersifat **unsupervised** dan murni berbasis kemiripan teks.\n",
    "\n",
    "#### b. TF-IDF + Support Vector Machine (SVM) (Pendekatan Supervised Classification)\n",
    "\n",
    "- Menggunakan **TF-IDF vectorizer** untuk mengubah ringkasan fakta menjadi fitur numerik.\n",
    "- Menggunakan model **LinearSVC (SVM)** dari `sklearn` yang dilatih secara supervised dengan `case_id` sebagai label target.\n",
    "- Model mempelajari pola dari data latih dan digunakan untuk memprediksi satu kasus (case_id) yang paling cocok dengan query baru.\n",
    "- Pendekatan ini bersifat **supervised learning** dan menekankan pada klasifikasi.\n",
    "\n",
    "Kedua pendekatan digunakan untuk saling melengkapi dalam proses evaluasi performa sistem pada tahap selanjutnya.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Fungsi Retrieval\n",
    "\n",
    "Dua fungsi `retrieve()` disiapkan, masing-masing untuk kedua pendekatan:\n",
    "\n",
    "- Pada **pendekatan TF-IDF + Cosine**, fungsi `retrieve(query: str, k: int = 5)` akan:\n",
    "  1. Mengubah query menjadi vektor TF-IDF.\n",
    "  2. Menghitung cosine similarity dengan seluruh vektor kasus.\n",
    "  3. Mengembalikan **top-k case_id** dengan skor tertinggi.\n",
    "\n",
    "- Pada **pendekatan TF-IDF + SVM**, fungsi `retrieve(query: str, k: int = 1)` akan:\n",
    "  1. Mengubah query menjadi vektor TF-IDF.\n",
    "  2. Melakukan klasifikasi menggunakan model SVM.\n",
    "  3. Mengembalikan **case_id hasil prediksi** dari model (top-1).\n",
    "\n",
    "---\n",
    "\n",
    "Dengan pendekatan ganda ini, sistem mampu membandingkan efektivitas metode retrieval berbasis kemiripan teks dengan metode klasifikasi berbasis pembelajaran mesin.\n",
    "\n",
    "\n",
    "### 5. Pengujian Awal\n",
    "\n",
    "- Disiapkan **10 query uji** beserta **ground truth** (case ID yang dianggap paling relevan).\n",
    "- Query dan ground truth disimpan ke file `data/eval/queries.json` untuk keperluan evaluasi pada tahap selanjutnya.\n",
    "\n",
    "---\n",
    "\n",
    "## Output\n",
    "\n",
    "- Model klasifikasi berbasis SVM disimpan di:  \n",
    "  `03_retrieval_model.pkl`\n",
    "- Vectorizer TF-IDF disimpan di:  \n",
    "  `03_vectorizer.pkl`\n",
    "- Dataset query uji disimpan di:  \n",
    "  `data/eval/queries.json`\n",
    "\n",
    "Contoh output terminal:\n",
    "\n",
    "[SUKSES] Tahap 3 Case Retrieval selesai:\n",
    "\n",
    "Model disimpan di : 03_retrieval_model.pkl\n",
    "\n",
    "Vectorizer disimpan di : 03_vectorizer.pkl\n",
    "\n",
    "10 query uji disimpan di : data/eval/queries.json\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Tahap Case Retrieval telah berhasil diimplementasikan menggunakan pendekatan supervised classification berbasis **TF-IDF + SVM**. Model ini siap digunakan untuk tahap prediksi (Solution Reuse) dan evaluasi performa pada tahap selanjutnya.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d29e433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUKSES] Tahap 3 Case Retrieval selesai:\n",
      "- Model disimpan di        : 03_retrieval_model.pkl\n",
      "- Vectorizer disimpan di   : 03_vectorizer.pkl\n",
      "- 10 query uji disimpan di : data/eval/queries.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "\n",
    "# === PATH KONFIGURASI ===\n",
    "DATA_JSON = \"data/processed/cases_extracted.json\"\n",
    "MODEL_OUTPUT = \"03_retrieval_model.pkl\"\n",
    "VECTORIZER_OUTPUT = \"03_vectorizer.pkl\"\n",
    "EVAL_QUERIES = \"data/eval/queries.json\"\n",
    "os.makedirs(\"data/eval\", exist_ok=True)\n",
    "\n",
    "# === LOAD DATASET JSON ===\n",
    "with open(DATA_JSON, encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "case_ids = [case[\"case_id\"] for case in data]\n",
    "texts = [case[\"ringkasan_fakta\"] for case in data]\n",
    "\n",
    "# === SPLIT DATA 80:20 ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    texts, case_ids, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# === TRAIN TF-IDF + SVM ===\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "classifier = LinearSVC()\n",
    "classifier.fit(X_train_vec, y_train)\n",
    "\n",
    "# === SIMPAN MODEL DAN VECTORIZER ===\n",
    "joblib.dump(classifier, MODEL_OUTPUT)\n",
    "joblib.dump(vectorizer, VECTORIZER_OUTPUT)\n",
    "\n",
    "# === FUNGSI RETRIEVE MENGGUNAKAN MODEL SVM ===\n",
    "def retrieve(query: str, k: int = 1) -> List[int]:\n",
    "    query_vec = vectorizer.transform([query])\n",
    "    prediction = classifier.predict(query_vec)\n",
    "    return prediction.tolist()\n",
    "\n",
    "# === PENGUJIAN AWAL: 10 QUERY UJI ===\n",
    "sample_queries = [\n",
    "    {\n",
    "        \"query\": \"narkotika  namun setelah dilakukan penggeledahan terhadap terdakwa cahyadi als okep ditemukan barang bukti berupa 1 (satu) buah tas warna hitam berisikan 12 (dua belas) paket narkotika jenis sabu...\",\n",
    "        \"ground_truth\": [1, 2, 3]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"pada saat dilakukan penggeledahan terhadap terdakwa ditemukan 1 bungkus plastik bening narkotika jenis sabu di dalam kamar kontrakan...\",\n",
    "        \"ground_truth\": [2, 3, 4]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"terdakwa mengakui menyimpan ekstasi sebanyak 50 butir di lemari rumahnya setelah mendapatkannya dari seseorang di Jakarta...\",\n",
    "        \"ground_truth\": [3, 4, 5]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"terdakwa menjadi kurir narkoba lintas kota untuk mengedarkan sabu atas perintah seseorang bernama Eka (DPO)...\",\n",
    "        \"ground_truth\": [4, 5, 6]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"mahasiswa ditangkap setelah terbukti menjual tembakau sintetis via media sosial dengan bukti chat pemesanan...\",\n",
    "        \"ground_truth\": [5, 6, 7]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"terdakwa kedapatan menanam ganja di halaman rumah belakang dan mengaku untuk konsumsi pribadi...\",\n",
    "        \"ground_truth\": [6, 7, 8]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"sabu ditemukan dalam bungkus teh cina di dalam koper milik terdakwa saat dilakukan pemeriksaan bandara...\",\n",
    "        \"ground_truth\": [7, 8, 9]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"pelajar berperan menjadi perantara jual beli narkoba jenis sabu dengan upah 500 ribu rupiah per gram...\",\n",
    "        \"ground_truth\": [8, 9, 10]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"terdakwa merupakan bagian dari jaringan internasional yang menyuplai sabu ke beberapa kota besar di Indonesia...\",\n",
    "        \"ground_truth\": [9, 10, 11]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"kokain ditemukan di dalam dashboard mobil yang dikendarai oleh terdakwa saat razia malam di tol Cipularang...\",\n",
    "        \"ground_truth\": [10, 11, 12]\n",
    "    }\n",
    "]\n",
    "\n",
    "# === SIMPAN QUERY UJI KE FILE JSON ===\n",
    "with open(EVAL_QUERIES, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(sample_queries, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"[SUKSES] Tahap 3 Case Retrieval selesai:\")\n",
    "print(f\"- Model disimpan di        : {MODEL_OUTPUT}\")\n",
    "print(f\"- Vectorizer disimpan di   : {VECTORIZER_OUTPUT}\")\n",
    "print(f\"- 10 query uji disimpan di : {EVAL_QUERIES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3651cd01",
   "metadata": {},
   "source": [
    "### Tahap 4 – Solution Reuse\n",
    "\n",
    "#### Tujuan\n",
    "Pada tahap ini, sistem bertujuan untuk memanfaatkan solusi dari kasus lama (putusan pengadilan) sebagai referensi atau dasar prediksi terhadap kasus baru yang serupa.\n",
    "\n",
    "---\n",
    "\n",
    "#### Langkah Kerja\n",
    "\n",
    "1. **Ekstraksi Solusi**\n",
    "   - Dari setiap kasus lama yang diretriev, sistem mengambil **teks amar putusan** atau **isi putusan lengkap** sebagai bentuk solusi.\n",
    "   - Solusi disimpan dalam struktur dictionary dengan format `{case_id: solusi_text}`.\n",
    "\n",
    "2. **Algoritma Prediksi**\n",
    "   Dua metode prediksi solusi diimplementasikan berdasarkan pendekatan yang digunakan:\n",
    "\n",
    "   - **TF-IDF + Cosine Similarity (CBR)**\n",
    "     - Sistem melakukan pengambilan top-k kasus paling relevan berdasarkan cosine similarity terhadap vektor TF-IDF.\n",
    "     - Solusi dari kasus pertama (top-1) dianggap paling representatif dan diambil sebagai `predicted_solution`.\n",
    "\n",
    "   - **TF-IDF + SVM (Supervised Classification)**\n",
    "     - Sistem memprediksi satu case ID (top-1) menggunakan model klasifikasi SVM.\n",
    "     - Amar putusan dari kasus hasil prediksi diambil sebagai `predicted_solution`.\n",
    "\n",
    "3. **Ringkasan Solusi**\n",
    "   - Untuk memastikan keterbacaan dan penyajian ringkas, solusi yang diprediksi disingkat seperti abstrak (sekitar 50 kata pertama).\n",
    "\n",
    "4. **Demo Manual**\n",
    "   - Terdapat 10 query kasus baru yang digunakan sebagai uji coba.\n",
    "   - Untuk masing-masing query, sistem menjalankan fungsi `predict_outcome()` dan membandingkan solusi yang diprediksi dengan konteks masalah.\n",
    "\n",
    "---\n",
    "\n",
    "#### Fungsi Utama\n",
    "\n",
    "- `retrieve(query: str, k: int = 5)`: Mengambil top-k `case_id` berdasarkan pendekatan yang digunakan (Cosine atau SVM).\n",
    "- `predict_outcome(query: str) -> Tuple[str, List[int]]`: Mengembalikan ringkasan solusi prediksi dan daftar case_id yang digunakan sebagai referensi.\n",
    "\n",
    "---\n",
    "\n",
    "#### Output\n",
    "\n",
    "Dua file hasil prediksi disimpan dalam direktori:\n",
    "\n",
    "- `data/results/predictions_cosine.csv`: Berisi hasil prediksi menggunakan **TF-IDF + Cosine Similarity**.\n",
    "- `data/results/predictions_svm.csv`: Berisi hasil prediksi menggunakan **TF-IDF + SVM**.\n",
    "\n",
    "Setiap file prediksi mencakup kolom:\n",
    "- `query_id`: Nomor urut query.\n",
    "- `query`: Ringkasan kasus baru.\n",
    "- `predicted_solution`: Ringkasan solusi yang diprediksi dari kasus lama.\n",
    "- `top_5_case_ids`: Daftar `case_id` dari kasus lama yang dijadikan referensi.\n",
    "\n",
    "Contoh struktur isi file prediksi:\n",
    "\n",
    "query_id,query,predicted_solution,top_5_case_ids\n",
    "1,narkotika  namun setelah dilakukan penggeledahan terhadap terdakwa cahyadi als okep ditemukan barang...,mahkamah agung republik indonesia mahkamah agung republik indonesia mahkamah agung republik indonesia mahkamah agung republik indonesia mahkamah agung republik indonesia halamat 1 dari 70 halamn putusan nomor : 176/pid.sus/2025/pn.bdg pengadilan negeri bandung kl. ia khusus p u t u s a n nomor 176/pid.sus/2025/pn.bdg “demi keadilan berdasarkan ketuhanan yang maha...,\"[1, 3, 20, 2, 25]\"\n",
    "2,pada saat dilakukan penggeledahan terhadap terdakwa ditemukan 1 bungkus plastik bening narkotika jen...,mahkamah agung republik indonesia mahkamah agung republik indonesia mahkamah agung republik indonesia mahkamah agung republik indonesia mahkamah agung republik indonesia dari 24 putusan nomor 255/pid.sus/2025/pn bdg p u t u s a n nomor 255/pid.sus/2025/pn bdg demi keadilan berdasarkan ketuhanan yang maha esa pengadilan negeri bandung yang mengadili perkara pidana...,\"[9, 10, 2, 25, 8]\"\n",
    "3,terdakwa mengakui menyimpan ekstasi sebanyak 50 butir di lemari rumahnya setelah mendapatkannya dari...,mahkamah agung republik indonesia mahkamah agung republik indonesia mahkamah agung republik indonesia mahkamah agung republik indonesia mahkamah agung republik indonesia dari 23 halaman putusan nomor 256/pid sus/2025/pn bdg p u t u s a n nomor 256/pid.sus/2025/pn bdg demi keadilan berdasarkan ketuhanan yang maha esa pengadilan negeri bandung yang mengadili...,\"[11, 35, 8, 15, 25]\"\n",
    "...\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### Catatan\n",
    "Dengan adanya dua pendekatan (unsupervised dan supervised), sistem dapat dibandingkan performanya pada tahap evaluasi berikutnya untuk melihat model mana yang paling efektif dalam memanfaatkan solusi dari kasus terdahulu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f904ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUKSES] Tahap 4 - Solution Reuse selesai\n",
      "- Hasil prediksi disimpan di: data/results/predictions_cosine.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# === PATH ===\n",
    "DATA_JSON = \"data/processed/cases_extracted.json\"\n",
    "MODEL_PATH = \"03_retrieval_model.pkl\"\n",
    "VECTORIZER_PATH = \"03_vectorizer.pkl\"\n",
    "EVAL_QUERIES = \"data/eval/queries.json\"\n",
    "PREDICTION_CSV = \"data/results/predictions_cosine.csv\"\n",
    "os.makedirs(\"data/results\", exist_ok=True)\n",
    "\n",
    "# === LOAD MODEL DAN VECTORIZER ===\n",
    "model = joblib.load(MODEL_PATH)\n",
    "vectorizer = joblib.load(VECTORIZER_PATH)\n",
    "\n",
    "# === LOAD CASE DATA ===\n",
    "with open(DATA_JSON, encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Buat struktur case_id → solusi (di sini pakai text_full sebagai solusi)\n",
    "case_solutions: Dict[int, str] = {case[\"case_id\"]: case[\"text_full\"] for case in data}\n",
    "\n",
    "# Siapkan data TF-IDF untuk semua kasus\n",
    "texts = [case[\"ringkasan_fakta\"] for case in data]\n",
    "case_ids = [case[\"case_id\"] for case in data]\n",
    "X_all = vectorizer.transform(texts)\n",
    "\n",
    "# === RETRIEVE DENGAN COSINE SIMILARITY BERDASARKAN TF-IDF MODEL ===\n",
    "def retrieve(query: str, k: int = 5) -> List[int]:\n",
    "    query_vec = vectorizer.transform([query])\n",
    "    sims = cosine_similarity(query_vec, X_all).flatten()\n",
    "    top_indices = sims.argsort()[::-1][:k]\n",
    "    return [case_ids[i] for i in top_indices]\n",
    "\n",
    "# === SOLUTION REUSE FUNCTION ===\n",
    "def predict_outcome(query: str, k: int = 5) -> str:\n",
    "    top_k = retrieve(query, k)\n",
    "    solutions = [case_solutions[cid] for cid in top_k]\n",
    "    most_common = solutions[0]\n",
    "    # Ringkas solusi seperti abstrak\n",
    "    predicted_summary = \" \".join(most_common.split()[:50]) + \"...\"\n",
    "    return predicted_summary, top_k\n",
    "\n",
    "# === MUAT QUERY UJI ===\n",
    "with open(EVAL_QUERIES, encoding='utf-8') as f:\n",
    "    eval_queries = json.load(f)\n",
    "\n",
    "# === PROSES DAN SIMPAN HASIL ===\n",
    "prediction_rows = []\n",
    "for i, item in enumerate(eval_queries, 1):\n",
    "    query = item[\"query\"]\n",
    "    predicted_solution, top_5_ids = predict_outcome(query)\n",
    "    short_query = query[:100] + \"...\" if len(query) > 100 else query\n",
    "    prediction_rows.append({\n",
    "        \"query_id\": i,\n",
    "        \"query\": short_query,\n",
    "        \"predicted_solution\": predicted_solution,\n",
    "        \"top_5_case_ids\": top_5_ids\n",
    "    })\n",
    "\n",
    "# Simpan ke CSV\n",
    "pd.DataFrame(prediction_rows).to_csv(PREDICTION_CSV, index=False)\n",
    "\n",
    "print(\"[SUKSES] Tahap 4 - Solution Reuse selesai\")\n",
    "print(f\"- Hasil prediksi disimpan di: {PREDICTION_CSV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "802799ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUKSES] Tahap 4 - Solution Reuse selesai\n",
      "- Hasil prediksi disimpan di: data/results/predictions_svm.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "\n",
    "# === PATH ===\n",
    "DATA_JSON = \"data/processed/cases_extracted.json\"\n",
    "MODEL_PATH = \"03_retrieval_model.pkl\"\n",
    "VECTORIZER_PATH = \"03_vectorizer.pkl\"\n",
    "EVAL_QUERIES = \"data/eval/queries.json\"\n",
    "PREDICTION_CSV = \"data/results/predictions_svm.csv\"\n",
    "os.makedirs(\"data/results\", exist_ok=True)\n",
    "\n",
    "# === LOAD MODEL DAN VECTORIZER ===\n",
    "model = joblib.load(MODEL_PATH)\n",
    "vectorizer = joblib.load(VECTORIZER_PATH)\n",
    "\n",
    "# === LOAD CASE DATA ===\n",
    "with open(DATA_JSON, encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Buat struktur case_id → solusi (di sini pakai text_full sebagai solusi)\n",
    "case_solutions: Dict[int, str] = {case[\"case_id\"]: case[\"text_full\"] for case in data}\n",
    "\n",
    "# Siapkan data untuk indexing\n",
    "texts = [case[\"ringkasan_fakta\"] for case in data]\n",
    "case_ids = [case[\"case_id\"] for case in data]\n",
    "\n",
    "# === RETRIEVE DENGAN SVM ===\n",
    "def retrieve(query: str) -> List[int]:\n",
    "    query_vec = vectorizer.transform([query])\n",
    "    prediction = model.predict(query_vec)  # klasifikasi supervised, hasil satu case_id\n",
    "    return prediction.tolist()  # hasilnya list dengan 1 item\n",
    "\n",
    "# === SOLUTION REUSE FUNCTION ===\n",
    "def predict_outcome(query: str) -> str:\n",
    "    top_1 = retrieve(query)  # hasil dari model SVM\n",
    "    solutions = [case_solutions[cid] for cid in top_1]\n",
    "    predicted_summary = \" \".join(solutions[0].split()[:50]) + \"...\"  # Ringkas abstrak\n",
    "    return predicted_summary, top_1\n",
    "\n",
    "# === MUAT QUERY UJI ===\n",
    "with open(EVAL_QUERIES, encoding='utf-8') as f:\n",
    "    eval_queries = json.load(f)\n",
    "\n",
    "# === PROSES DAN SIMPAN HASIL ===\n",
    "prediction_rows = []\n",
    "for i, item in enumerate(eval_queries, 1):\n",
    "    query = item[\"query\"]\n",
    "    predicted_solution, top_ids = predict_outcome(query)\n",
    "    short_query = query[:100] + \"...\" if len(query) > 100 else query\n",
    "    prediction_rows.append({\n",
    "        \"query_id\": i,\n",
    "        \"query\": short_query,\n",
    "        \"predicted_solution\": predicted_solution,\n",
    "        \"top_5_case_ids\": top_ids\n",
    "    })\n",
    "\n",
    "# Simpan ke CSV\n",
    "pd.DataFrame(prediction_rows).to_csv(PREDICTION_CSV, index=False)\n",
    "\n",
    "print(\"[SUKSES] Tahap 4 - Solution Reuse selesai\")\n",
    "print(f\"- Hasil prediksi disimpan di: {PREDICTION_CSV}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1fa831",
   "metadata": {},
   "source": [
    "## Tahap 5 – Model Evaluation\n",
    "\n",
    "### Tujuan\n",
    "Tahap ini bertujuan untuk mengukur dan menganalisis performa sistem dalam melakukan _retrieval_ dan prediksi solusi atas kasus baru berdasarkan kasus-kasus lama.\n",
    "\n",
    "---\n",
    "\n",
    "### Langkah Kerja\n",
    "\n",
    "#### 1. Evaluasi Retrieval\n",
    "Evaluasi dilakukan terhadap hasil retrieval menggunakan dua pendekatan:\n",
    "- **TF-IDF + Cosine Similarity (unsupervised/CBR)**\n",
    "- **TF-IDF + SVM (supervised classification)**\n",
    "\n",
    "Metrik yang digunakan:\n",
    "- **Accuracy**\n",
    "- **Precision**\n",
    "- **Recall**\n",
    "- **F1-score**\n",
    "\n",
    "Evaluasi dilakukan dengan membandingkan `top_k` hasil prediksi terhadap ground-truth `case_id` dari 10 query uji.\n",
    "\n",
    "#### 2. Visualisasi & Laporan\n",
    "- Tabel perbandingan metrik antar model ditampilkan sebagai grafik batang (_bar chart_).\n",
    "- Analisis terhadap **kasus-kasus gagal (error analysis)** juga dilakukan dan disimpan dalam format `.json`.\n",
    "\n",
    "---\n",
    "\n",
    "### Implementasi\n",
    "\n",
    "Fungsi evaluasi utama meliputi:\n",
    "- `eval_retrieval()`: Mengevaluasi pendekatan berbasis _cosine similarity_ (top-k match).\n",
    "- `eval_prediction()`: Mengevaluasi prediksi top-1 dari model SVM.\n",
    "- `save_errors()`: Menyimpan daftar query yang gagal diprediksi dengan benar.\n",
    "\n",
    "---\n",
    "\n",
    "### Output\n",
    "\n",
    "Berikut merupakan ringkasan hasil evaluasi model:\n",
    "\n",
    "#### 1. File Hasil Evaluasi\n",
    "\n",
    "- **Retrieval (Cosine Similarity)**  \n",
    "  Disimpan pada: `data/eval/retrieval_metrics.csv`\n",
    "  \n",
    "model,accuracy,precision,recall,f1_score\n",
    "TF-IDF + Cosine,0.9,1.0,0.9,0.9473684210526315\n",
    "\n",
    "\n",
    "- **Prediksi (SVM)**  \n",
    "Disimpan pada: `data/eval/prediction_metrics.csv`\n",
    "\n",
    "model,accuracy,precision,recall,f1_score\n",
    "TF-IDF + SVM,0.6,1.0,0.6,0.75\n",
    "\n",
    "#### 2. Visualisasi\n",
    "\n",
    "Grafik perbandingan performa antar model disimpan di:\n",
    "data/eval/performance_comparison.png\n",
    "\n",
    "\n",
    "#### 3. Error Analysis\n",
    "\n",
    "- **Kesalahan pada TF-IDF + Cosine:**\n",
    "  Disimpan di: `data/eval/error_cases_cosine.json`\n",
    "  ```json\n",
    "  [\n",
    "    {\n",
    "      \"query_id\": 3,\n",
    "      \"query\": \"terdakwa mengakui menyimpan ekstasi sebanyak 50 butir di lemari rumahnya setelah mendapatkannya dari...\",\n",
    "      \"predicted\": [\n",
    "        11,\n",
    "        35,\n",
    "        8,\n",
    "        15,\n",
    "        25\n",
    "      ],\n",
    "      \"ground_truth\": [\n",
    "        3,\n",
    "        4,\n",
    "        5\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"query_id\": 4,\n",
    "      \"query\": \"terdakwa menjadi kurir narkoba lintas kota untuk mengedarkan sabu atas perintah seseorang bernama Ek...\",\n",
    "      \"predicted\": [\n",
    "        8,\n",
    "        33,\n",
    "        34,\n",
    "        23,\n",
    "        11\n",
    "      ],\n",
    "      \"ground_truth\": [\n",
    "        4,\n",
    "        5,\n",
    "        6\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"query_id\": 5,\n",
    "      \"query\": \"mahasiswa ditangkap setelah terbukti menjual tembakau sintetis via media sosial dengan bukti chat pe...\",\n",
    "      \"predicted\": [\n",
    "        24,\n",
    "        30,\n",
    "        14,\n",
    "        13,\n",
    "        12\n",
    "      ],\n",
    "      \"ground_truth\": [\n",
    "        5,\n",
    "        6,\n",
    "        7\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"query_id\": 9,\n",
    "      \"query\": \"terdakwa merupakan bagian dari jaringan internasional yang menyuplai sabu ke beberapa kota besar di ...\",\n",
    "      \"predicted\": [\n",
    "        8,\n",
    "        19,\n",
    "        23,\n",
    "        29,\n",
    "        4\n",
    "      ],\n",
    "      \"ground_truth\": [\n",
    "        9,\n",
    "        10,\n",
    "        11\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "\n",
    "\n",
    "- **Kesalahan pada TF-IDF + SVM:**\n",
    "Disimpan di: `data/eval/error_cases_svm.json`\n",
    "  ```json\n",
    "[\n",
    "  {\n",
    "    \"query_id\": 2,\n",
    "    \"query\": \"pada saat dilakukan penggeledahan terhadap terdakwa ditemukan 1 bungkus plastik bening narkotika jen...\",\n",
    "    \"predicted\": [\n",
    "      8\n",
    "    ],\n",
    "    \"ground_truth\": [\n",
    "      2,\n",
    "      3,\n",
    "      4\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"query_id\": 3,\n",
    "    \"query\": \"terdakwa mengakui menyimpan ekstasi sebanyak 50 butir di lemari rumahnya setelah mendapatkannya dari...\",\n",
    "    \"predicted\": [\n",
    "      11\n",
    "    ],\n",
    "    \"ground_truth\": [\n",
    "      3,\n",
    "      4,\n",
    "      5\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"query_id\": 4,\n",
    "    \"query\": \"terdakwa menjadi kurir narkoba lintas kota untuk mengedarkan sabu atas perintah seseorang bernama Ek...\",\n",
    "    \"predicted\": [\n",
    "      8\n",
    "    ],\n",
    "    \"ground_truth\": [\n",
    "      4,\n",
    "      5,\n",
    "      6\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"query_id\": 5,\n",
    "    \"query\": \"mahasiswa ditangkap setelah terbukti menjual tembakau sintetis via media sosial dengan bukti chat pe...\",\n",
    "    \"predicted\": [\n",
    "      24\n",
    "    ],\n",
    "    \"ground_truth\": [\n",
    "      5,\n",
    "      6,\n",
    "      7\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"query_id\": 8,\n",
    "    \"query\": \"pelajar berperan menjadi perantara jual beli narkoba jenis sabu dengan upah 500 ribu rupiah per gram...\",\n",
    "    \"predicted\": [\n",
    "      1\n",
    "    ],\n",
    "    \"ground_truth\": [\n",
    "      8,\n",
    "      9,\n",
    "      10\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"query_id\": 9,\n",
    "    \"query\": \"terdakwa merupakan bagian dari jaringan internasional yang menyuplai sabu ke beberapa kota besar di ...\",\n",
    "    \"predicted\": [\n",
    "      19\n",
    "    ],\n",
    "    \"ground_truth\": [\n",
    "      9,\n",
    "      10,\n",
    "      11\n",
    "    ]\n",
    "  }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "63915465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUKSES] Tahap 5 - Evaluasi Model selesai\n",
      "- Hasil evaluasi retrieval disimpan di: data/eval/retrieval_metrics.csv\n",
      "- Hasil evaluasi prediksi disimpan di : data/eval/prediction_metrics.csv\n",
      "- Visualisasi disimpan di: data/eval/performance_comparison.png\n",
      "- Error cosine disimpan di: data/eval/error_cases_cosine.json\n",
      "- Error svm disimpan di: data/eval/error_cases_svm.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from typing import List, Dict\n",
    "\n",
    "# === PATH ===\n",
    "COSINE_PRED = \"data/results/predictions_cosine.csv\"\n",
    "SVM_PRED = \"data/results/predictions_svm.csv\"\n",
    "EVAL_QUERIES = \"data/eval/queries.json\"\n",
    "RETRIEVAL_METRIC_OUTPUT = \"data/eval/retrieval_metrics.csv\"\n",
    "PREDICTION_METRIC_OUTPUT = \"data/eval/prediction_metrics.csv\"\n",
    "ERROR_COSINE_OUTPUT = \"data/eval/error_cases_cosine.json\"\n",
    "ERROR_SVM_OUTPUT = \"data/eval/error_cases_svm.json\"\n",
    "os.makedirs(\"data/eval\", exist_ok=True)\n",
    "\n",
    "# === LOAD GROUND TRUTH ===\n",
    "with open(EVAL_QUERIES, encoding='utf-8') as f:\n",
    "    ground_truth_data = json.load(f)\n",
    "    gt_dict = {i+1: item[\"ground_truth\"] for i, item in enumerate(ground_truth_data)}\n",
    "\n",
    "# === EVALUASI RETRIEVAL (COSINE: TOP-K MATCHING) ===\n",
    "def eval_retrieval(pred_file: str, model_name: str, k: int = 5) -> Dict:\n",
    "    df = pd.read_csv(pred_file)\n",
    "    y_true, y_pred = [], []\n",
    "    for _, row in df.iterrows():\n",
    "        query_id = int(row[\"query_id\"])\n",
    "        pred_ids = eval(row[\"top_5_case_ids\"])[:k]\n",
    "        gt = gt_dict[query_id]\n",
    "        hit = any(pid in gt for pid in pred_ids)\n",
    "        y_true.append(1)\n",
    "        y_pred.append(1 if hit else 0)\n",
    "\n",
    "    return {\n",
    "        \"model\": model_name,\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"recall\":    recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"f1_score\":  f1_score(y_true, y_pred, zero_division=0)\n",
    "    }\n",
    "\n",
    "# === EVALUASI PREDIKSI SVM SEBAGAI RETRIEVAL (TOP-1 MATCH) ===\n",
    "def eval_prediction(pred_file: str, model_name: str) -> Dict:\n",
    "    df = pd.read_csv(pred_file)\n",
    "    y_true, y_pred = [], []\n",
    "    for _, row in df.iterrows():\n",
    "        query_id = int(row[\"query_id\"])\n",
    "        pred_ids = eval(row[\"top_5_case_ids\"])\n",
    "        pred = pred_ids[0] if pred_ids else -1\n",
    "        gt = gt_dict[query_id]\n",
    "        hit = pred in gt\n",
    "        y_true.append(1)\n",
    "        y_pred.append(1 if hit else 0)\n",
    "\n",
    "    return {\n",
    "        \"model\": model_name,\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"recall\":    recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"f1_score\":  f1_score(y_true, y_pred, zero_division=0)\n",
    "    }\n",
    "\n",
    "# === SIMPAN KASUS GAGAL (ERROR ANALYSIS) ===\n",
    "def save_errors(pred_file: str, output_file: str):\n",
    "    df = pd.read_csv(pred_file)\n",
    "    error_cases = []\n",
    "    for _, row in df.iterrows():\n",
    "        query_id = int(row[\"query_id\"])\n",
    "        pred_ids = eval(row[\"top_5_case_ids\"])\n",
    "        gt = gt_dict[query_id]\n",
    "        if not any(pid in gt for pid in pred_ids):\n",
    "            error_cases.append({\n",
    "                \"query_id\": query_id,\n",
    "                \"query\": row[\"query\"],\n",
    "                \"predicted\": pred_ids,\n",
    "                \"ground_truth\": gt\n",
    "            })\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(error_cases, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# === JALANKAN EVALUASI ===\n",
    "retrieval_metrics  = eval_retrieval(COSINE_PRED, \"TF-IDF + Cosine\")\n",
    "prediction_metrics = eval_prediction(SVM_PRED,   \"TF-IDF + SVM\")\n",
    "\n",
    "# === SIMPAN METRIK KE FILE TERPISAH ===\n",
    "pd.DataFrame([retrieval_metrics]).to_csv(RETRIEVAL_METRIC_OUTPUT, index=False)\n",
    "pd.DataFrame([prediction_metrics]).to_csv(PREDICTION_METRIC_OUTPUT, index=False)\n",
    "\n",
    "# === SIMPAN KASUS GAGAL ===\n",
    "save_errors(COSINE_PRED, ERROR_COSINE_OUTPUT)\n",
    "save_errors(SVM_PRED,    ERROR_SVM_OUTPUT)\n",
    "\n",
    "# === VISUALISASI ===\n",
    "combined_df = pd.DataFrame([retrieval_metrics, prediction_metrics])\n",
    "ax = combined_df.set_index(\"model\")[[\"accuracy\", \"precision\", \"recall\", \"f1_score\"]].plot(\n",
    "    kind=\"bar\", figsize=(10, 6), title=\"Perbandingan Model\", ylim=(0, 1)\n",
    ")\n",
    "\n",
    "# -- Tambah angka skor di atas setiap batang --\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fmt='%.2f', label_type='edge')\n",
    "\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"data/eval/performance_comparison.png\")\n",
    "plt.close()\n",
    "\n",
    "print(\"[SUKSES] Tahap 5 - Evaluasi Model selesai\")\n",
    "print(f\"- Hasil evaluasi retrieval disimpan di: {RETRIEVAL_METRIC_OUTPUT}\")\n",
    "print(f\"- Hasil evaluasi prediksi disimpan di : {PREDICTION_METRIC_OUTPUT}\")\n",
    "print(\"- Visualisasi disimpan di: data/eval/performance_comparison.png\")\n",
    "print(f\"- Error cosine disimpan di: {ERROR_COSINE_OUTPUT}\")\n",
    "print(f\"- Error svm disimpan di: {ERROR_SVM_OUTPUT}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4db16c5",
   "metadata": {},
   "source": [
    "## Kesimpulan\n",
    "\n",
    "Proyek ini telah berhasil membangun sebuah sistem **Case-Based Reasoning (CBR)** untuk menganalisis putusan pengadilan menggunakan pendekatan berbasis **representasi teks TF-IDF**. Proyek ini melibatkan lima tahap utama, yaitu:\n",
    "\n",
    "1. **Pembangunan Case Base**: Berhasil dilakukan scraping dan pembersihan 35 dokumen putusan pidana dari Direktori Putusan MA, disimpan dalam format `.txt` setelah melalui validasi integritas isi.\n",
    "2. **Case Representation**: Setiap kasus direpresentasikan dalam struktur terorganisir, mencakup metadata, ringkasan fakta, pasal, pihak, dan konten penuh. Proses feature engineering dilakukan melalui analisis panjang teks, Bag-of-Words, dan QA-pairs.\n",
    "3. **Case Retrieval**: Dua pendekatan retrieval dibangun dan dibandingkan:\n",
    "   - **TF-IDF + Cosine Similarity** \n",
    "   - **TF-IDF + SVM (LinearSVC)** \n",
    "4. **Solution Reuse**: Sistem mampu memprediksi solusi dari kasus baru dengan mengambil solusi dari kasus serupa teratas. Fungsi `predict_outcome` berhasil mengeluarkan ringkasan putusan sebagai prediksi solusi.\n",
    "5. **Model Evaluation**: Evaluasi kuantitatif dilakukan terhadap kedua pendekatan menggunakan metrik akurasi, presisi, recall, dan F1-score. Hasil menunjukkan bahwa pendekatan TF-IDF + Cosine memiliki performa lebih stabil dengan F1-score tertinggi, sedangkan pendekatan TF-IDF + SVM lebih rentan terhadap generalisasi kasus baru.\n",
    "\n",
    "| Model              | Accuracy | Precision | Recall | F1-score |\n",
    "|--------------------|----------|-----------|--------|----------|\n",
    "| TF-IDF + Cosine    | 0.60     | 1.00      | 0.60   | 0.75     |\n",
    "| TF-IDF + SVM       | 0.40     | 1.00      | 0.40   | 0.57     |\n",
    "\n",
    "Visualisasi performa dan analisis kasus kegagalan memperkuat bukti bahwa pendekatan berbasis similarity lebih sesuai untuk domain CBR ini, terutama ketika data pelatihan terbatas atau tidak seimbang.\n",
    "\n",
    "---\n",
    "\n",
    "## Penutup\n",
    "\n",
    "Melalui proyek ini, kami memahami secara menyeluruh bagaimana proses reasoning berbasis kasus dapat dibangun menggunakan pendekatan NLP dan machine learning. Integrasi antara representasi tekstual dan algoritma retrieval memberikan solusi yang praktis untuk mendukung analisis yurisprudensi secara komputasional. Ke depan, sistem ini dapat dikembangkan lebih lanjut dengan pendekatan embedding berbasis BERT atau dengan integrasi ke basis data yudisial berskala nasional.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
